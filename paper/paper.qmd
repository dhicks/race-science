---
title: "Race Science in Mainstream Psychology, 1960-2010"
authors:
  - name: "D.J. Hicks"
    email: "dhicks4@ucmerced.edu"
    affiliations: 
      - id: a
        name: "University of California, Merced"
        city: "Merced"
        country: "CA, USA"
    orcid: "0000-0001-7945-4416"
    corresponding: true
  - name: "Emilio J. C. Lobato"
    affiliations:
      - ref: a
    orcid: "0000-0002-3066-2932"

abstract: |
  | Anthropologists and human geneticists have long rejected efforts to purportedly scientifically justify racial inequality and colonialism.  But race science has persisted, at least on the margins of the mainstream scientific community.  In this study, scientometric and text mining methods were used to trace the appearance of race science discourse in both mainstream behavioral science journals and the parascholarly journal *Mankind Quarterly* from 1960 to 2010.  Our analysis found two distinguishable lines of race science discourse.  One is strongly associated with *Mankind Quarterly*, while the other — emphasizing race and intelligence — is more prevalent in mainstream psychology journals, especially *Intelligence* and *Personality and Individual Differences*.  We compare psychology's response to race science to those of other fields, and discuss the social responsibility of scientists with respect to race science. 
  
significance: |
  | Anthropologists and human geneticists have long rejected efforts to purportedly scientifically justify racial inequality and colonialism.  This study examines the persistence of scientific racism throughout the second half of the twentieth century, examining both mainstream behavioral science journals and a notorious fringe journal, *Mankind Quarterly* (MQ).  While we expected to find evidence that race science emerged first in MQ and later spread to mainstream journals, we instead found two distinct lines of race science discourse.  Race-and-intelligence discourse appears first and primarily in two mainstream journals, *Intelligence* and *Personality and Individual Differences*. Unlike anthropology and human genetics, psychology has not strongly condemned race science in the mainstream of the field.  
  
keywords: 
  - race science
  - race and intelligence
  - psychology
  - behavior genetics
  - text mining
  - topic modeling
  
bibliography: [references.yaml, refs-manual.bib]

format: 
  aft-pdf:
    keep-tex: true
    link-citations: true
    mathspec: true
    fig.env: "figure*"
    number-sections: true
    cite-method: citeproc
    include-in-header: 
        text: |
            \usepackage{lineno}
            \linenumbers
    geometry:
        - left = 1.5in
        - right = 1.5in

#  elsevier-pdf:  # https://github.com/quarto-journals/elsevier
#    keep-tex: true
#    link-citations: true
#    mathspec: true
#    fig.env: "figure*"
#    number-sections: true
#    journal:
#      name: "NA"
#      cite-style: "authoryear"
#      layout: "onecolumn"
#    include-in-header: 
#      text: |
#        \usepackage{lineno}
#        \linenumbers
#    cite_method: "citeproc"
    
crossref:
  fig-prefix: ""
  tbl-prefix: ""

execute:
  cache: true
---

<!-- 
QSS submission guidelines: 
<https://direct.mit.edu/qss/pages/submission-guidelines>

- 300-word cover letter on significance and fit
- Articles: original theoretical, empirical, or methodological research (typically between 5,000 and 8,000 words)

 -->
 
# Introduction

By the end of the Second World War, a combination of technical developments in anthropology and genetics, intellectual campaigns by the likes of Franz Boas and W.E.B. Du Bois, and revelations of Nazi atrocities meant that the dominant position within the scientific community rejected attempts to scientifically justify racial inequalities and colonialism [@BarkanRetreatScientificRacism1992; @YudellRaceUnmaskedBiology2014; @JacksonDarwinismDemocracyRace2017].  But eugenic thinking and scientific racism persisted throughout the second half of the twentieth century, and recent work has found that white supremacists continue to attempt to use scientific research to justify racial violence [@DusterBackdoorEugenics2003; @JacksonScienceSegregationRace2005; @PanofskyHowWhiteNationalists2021; @BirdTypologicalThinkingHuman2024; @PronczukRacistResearcherExposed2022].  

The aim of the study reported here was to trace race science in the scholarly and parascholarly literature, examining the ways that fringe ideas can present themselves in mainstream venues as legitimate scholarly inquiry.  

We first propose a distinction between scientific racism, race science, and race science discourse.  *Scientific racism* refers to the social practice of purporting to justify racial inequality and colonialism by appealing to the epistemic authority of science.  *Race science* refers to scientific and/or pseudoscientific research, including research products (journal articles, etc.), that can be utilized for scientific racism.  *Race science discourse* refers to a broader category, any treatment of race science as a legitimate area of scientific research.  (@sec-operationalizing discusses how this conception of race science might be operationalized in practice. @BirdTypologicalThinkingHuman2024 give a concrete example of how a particular data visualization was transformed into a scientific racist meme.) 

We stress that race science discourse includes methodological and empirical critiques of race science.  We argue that such technical critiques — even when they are intended to undermine race science — can instead legitimize it, by creating an impression among the broader public that race science is a matter of ordinary, reasonable, mainstream scientific debate.  For example, the Flynn effect (the finding that IQ scores have increased over time) poses a serious empirical challenge to hereditarian claims of racial differences in intelligence, and has been extensively debated in the mainstream scientific literature.  Most members of the general public lack the time and skills needed to assess these debates for themselves.  So, we suggest, they are quite likely to use a "middle ground" heuristic to understand these debates as ordinary, reasonable disagreement among mainstream scientists.  Some experts say one thing, other experts say something else, and so "the truth is somewhere in the middle."  In this way technical critiques of race science based on the Flynn effect could legitimize race science.  So, while empirical research supporting the Flynn effect is usually not itself race science, it can still be race science discourse.  

The distinction between scientific racism, race science, and race science discourse allows us to bracket the intentions and attitudes of the researchers involved in any particular piece of research or debate.  For the purposes of the current study, it is not important whether, for instance, psychologist Arthur Jensen is or is not correctly labelled a scientific racist [@JacksonJr.ArthurJensenEvolutionary2022].  But it is important that much of Jensen's research was useful and indeed utilized to purportedly justify racial inequality, and so unambiguously qualifies as race science.  

<!-- ## Scientific racism, eugenics, and *Brown*  -->

Histories of scientific racism in the twentieth century have often emphasized the Pioneer Fund and the parascholarly journal *Mankind Quarterly* (MQ) [
@BarkanRetreatScientificRacism1992;
@MehlerFoundationFascismNew1989;
@WinstonScienceServiceFar1998;
@TuckerFundingScientificRacism2002;
@SchafferScientificRacismAgain2007;
@SainiSuperiorReturnRace2019;
@WinstonScientificRacismNorth2020;
@AdamsMisappropriationBiologicalAnthropology2021;
@SainiDraperMillionsPhilanthropic2022].  Pioneer was formed in 1937, during the waning years of the eugenics movement in North America [@BarkanRetreatScientificRacism1992; @TuckerFundingScientificRacism2002], with the aim of "support[ing] academic research and the \'dissemination of information, into the 'problem of heredity and eugenics' and 'the problems of race betterment'" [@MehlerFoundationFascismNew1989 p.21, quoting Laughlin, unbalanced quotation marks in Mehler].  MQ was founded in 1960 by biologist R. Ruggles Gates (1882-1962), psychologist Henry Garrett (1894-1973), and non-academic anthropologist G. Robert Gayre (self-styled as "Gayre of Gayre and Nigg"; 1907-1996).  Gates' professional status had risen and fallen with eugenics and the explicit scientific racism of the 1920s, and by the end of the Second World War he was thoroughly marginalized [@WinstonScienceServiceFar1998].  In contrast, Garrett had been president of the American Psychological Association in 1946 and chair of Psychology at Columbia from 1941 to 1955 [@WinstonScienceServiceFar1998].  [@ShukmanRaceScienceInc2024 report that the Pioneer Fund has been effectively renamed the Human Diversity Foundation (HDF).] 

In the landmark case *Brown v Board of Education of Topeka* (1954), the US Supreme Court banned *de jure* educational segregation.  The Court's decision relied on expert testimony from psychologists and education researchers; but segregationists also put forward their own experts, including Henry Garrett [@WinstonScienceServiceFar1998; @JacksonScienceSegregationRace2005; @SchafferScientificRacismAgain2007].  After *Brown*, Pioneer funded various segregationist efforts across the United States, including lectures on eugenics by Stanford physicist William Shockley [@TuckerFundingScientificRacism2002; @JacksonScienceSegregationRace2005].  MQ was created to provide a favorable venue for race scientists to publish their views, on the grounds that an "equalitarian dogma" created a censorious "taboo" against their research in mainstream publications [@TuckerFundingScientificRacism2002; @JacksonMythicalTabooRace2020].  

Scholars in the agnotology ("theory of ignorance") literature have examined the ways that powerful economic interests — especially the fossil fuel and tobacco industries — have "manufactured doubt," creating an inaccurate public perception of scientific uncertainty to delay public protective regulation [@OreskesMerchantsDoubtHow2011; @FernandezPintoKnowBetterNot2017; @KouranyScienceProductionIgnorance2020]. Following @OreskesMerchantsDoubtHow2011, @FernandezPintoKnowBetterNot2017 identified five "core strategies or mechanisms" of the "tobacco strategy," showing how this strategy was used to manufacture doubt surrounding climate change, pharmaceuticals, and the causes of the 2008 financial crisis.  

Race science has not been heavily studied using an agnotology framework [though see @MillsRacialContract2014; @KouranyMightScientificIgnorance2020; @DasguptaBiologicalEssentialismWhite2024; @DasguptaRacistAgnotologyHow2025].  The qualitative historiography suggests that actors such as the Pioneer Fund have deployed a version of the tobacco strategy to promote misleading public perceptions of scientific uncertainty about (or even scientific support for) racial hierarchy. (Indeed, the origins of the "tobacco strategy" are often dated to a 1954 advertising campaign by the tobacco industry, 17 years after Pioneer was founded. Scientific racists and the eugenics movement might have been deploying the "tobacco strategy" for decades before the tobacco industry.) 

Considering the five strategies identified by @FernandezPintoKnowBetterNot2017, the Pioneer Fund itself likely exemplifies the strategy of supporting friendly research; and figures such as Shockley and Jensen exemplify the strategy of recruiting high-profile, distinguished scientists [@TuckerFundingScientificRacism2002]. 

In @FernandezPintoKnowBetterNot2017, the paradigm for supporting friendly research is the Tobacco Industry Research Committee, later known as the Council for Tobacco Research. @FernandezPintoKnowBetterNot2017 points out that the TIRC/CTR funded "scientific experts in major universities and research institutions" rather than marginal, fringe, or industry-employed scientists (57). The purpose was "to use 'good' research as a distraction" from the harmful effects of smoking by "fund[ing] research on heredity, infection, nutrition, hormones, nervous tension, and environmental factors" as causes of lung cancer and heart disease (57). This process of "industrial selection" for favorable research [@HolmanExperimentationIndustrialSelection2017] created the misleading impression that these non-tobacco causes were just as important, perhaps even more important, than smoking [compare @WeatherallHowBeatScience2018; @OConnorMisinformationAgeHow2019]. Similarly, per the quote from Laughlin above, the Pioneer Fund was all-but-explicitly created to promote research on eugenics and race science as just as important, if not more important, than non-biological ("environmentalist") explanations of racial social inequality. 

For the current project, we hypothesized that MQ and certain mainstream journals played a key role in another strategy identified by @FernandezPintoKnowBetterNot2017, the "echo chamber effect." We theorize this effect as involving three components: the "margins," the "mainstream," and the "bridge" connecting them.  @FernandezPintoKnowBetterNot2017 argues that, using sponsored journals and conferences, "the tobacco industry circumvented peer review standards in publication, without compromising its façade of research supporter" (59). These kinds of marginal venues provide a "safe space" in which favorable conceptual frameworks and empirical research (or pseudo-research) could be developed, protected from the destructive critical scrutiny they would receive in the mainstream [@HicksYoungsPValuePlot2022]. Bridge venues are accepted as part of the mainstream, but cite and draw on work from the margins, thereby mainstreaming these marginal ideas, that is, making them appear to be part of the legitimate body of mainstream scientific research. Where supporting friendly research is a strategy more-or-less entirely within the mainstream of the scientific community, the "echo chamber" strategy is located on the margins or fringes. 

MQ does not appear to have ever been considered a mainstream journal; it has always existed firmly on the margins. If MQ provided the "safe space" for developing race science, protected away from mainstream criticism, then bridge venues would be necessary to disseminate these ideas into the scientific mainstream. We therefore hypothesized that (H1) scientometric and text mining analyses would show race science discourse appearing first and most prominently in MQ, and appearing in mainstream publications only subsequently, with certain venues serving as the bridge.  

Specifically, among both (other) academics and the general public, the field of behavior genetics is strongly associated with race science [@PanofskyMisbehavingScienceControversy2014]. We therefore hypothesized that (H2) the journal *Behavior Genetics* (BG) would be a significant bridge venue for the dissemination of race science discourse.  

Neither of these hypotheses were supported. Instead, mainstream psychology journals provided a venue for a distinct form of race science discourse, based on intelligence research and originating in two mainstream journals — *Intelligence* and *Personality and Individual Differences* — rather than MQ. And BG has not been a significant venue for either form of race science discourse. 


# Methods and materials 

The primary data for this study were the full text of all available articles published in journals that had published multiple Pioneer-funded researchers, between 1960 (the year MQ was founded) and 2010.  The end year was chosen for a combination of reasons: easy access to MQ ended at 2004, and we did not want to extend the data too far beyond that point; 2010 gave us a "tidy" 50-year window of coverage; and an end year that wasn't too recent would hopefully allow us to avoid distracting controversies about researchers who are still active today. 

## Corpus assembly 

Corpus assembly is summarized in figure @fig-corpus.  The corpus was assembled in two parts, one for *Mankind Quarterly* and the other for mainstream journals.  


### Mankind Quarterly 

An electronic archive of *Mankind Quarterly*, from the first issue in 1960 through 2004, is available for free on the open web at the white nationalist website *The Unz Review*.  The Python package Beautiful Soup [@BeautifulSoupScreenscrapingLibrary, version not recorded] was used to retrieve every PDF available in this archive.  After identifying some gaps (missing issues) in the archive from *The Unz Review*, we manually retrieved PDFs for further articles using ProQuest.  Text was extracted using the R package `pdftools` [@OomsPdftoolsTextExtraction2023 version 3.0.1].  This stage of corpus assembly was conducted in fall 2021 with the assistance of Anthony Sainez. 

### Mainstream journals 

To identify suitable mainstream journals for inclusion in the corpus, we first identified academic researchers who had been funded by the Pioneer Fund.  We reviewed a critical profile of Pioneer [@MillerPioneerFundBankrolling1994] as well as an archived page from the organization's own web site (<https://web.archive.org/web/20130103005545/http://www.pioneerfund.org/Grantees.html>), which together listed 16 researchers who had received Pioneer funds.  

We then used the author search tool in Clarivate's Web of Science platform (<https://www.webofscience.com/wos/author/search>), retrieving publication lists for 14 researchers.  These searches were conducted between 2021-09-24 and 2021-10-05 by DJH.  After parsing these results, we counted how many of the 14 researchers had published in each journal.  Thirteen journals had published 6 or more of the 14 Pioneer-funded researchers.  See table @tbl-researchers and figure @fig-wos.  

| name (birth-death) | field | affiliation | WoS doc. count |
|:--------|:----|:--------|-:|
| Thomas J. Bouchard, Jr. (1937-) | psychology | Univ. Minnesota | 184 |
| Brunetto Chiarelli (?-?) | anthropology | | 91 |
| Hans Eysenck (1916-1997) | psychology | King's College London | 661 |
| Robert A. Gordon (1932-) | sociology | Johns Hopkins | 0|
| Linda Gottfredson (1947-) | psychology | Univ. Delaware | 71 |
| Garrett Hardin (1915-2003) | ecology | UC Santa Barbara | 75 |
| Joseph M. Horn (1940-) | psychology | UT Austin | 68 |
| Lloyd Humphreys (1913-2003) | psychology | Univ. Illinois | 0 |
| Arthur Jensen (1923-2003) | psychology | UC Berkeley | 235 |
| Michael Levin (1943-) | philosophy | CCNY | 96 |
| Richard Lynn (1930-2023) | psychology | Ulster University\* | 288 |
| R. Travis Osborne (1913-2013) | psychology | Univ. Georgia | 59 |
| J. Phillippe Rushton (1943-2012) | psychology | Univ. Western Ontario | 277 |
| Audrey M. Shuey (1900-1977) | psychology | Randolph-Macon Women's College | 10 |
| Philip A. Vernon (1950-) | psychology | Univ. Western Ontario | 227 |
| Daniel Vining, Jr. (1944-) | demography | Univ. Pennsylvania | 33 |

Table: Pioneer-funded researchers.  Either identified in [@MillerPioneerFundBankrolling1994] or named on an archive copy of Pioneer's website; along with birth and death dates from Wikipedia, attributed discipline, academic affiliation, and WoS author search result counts.  Brunetto Chiarelli does not have a Wikipedia page; Richard Lynn's title of Professor Emeritus at Ulster University was withdrawn in 2018. {#tbl-researchers}

![Journals publishing 6 or more Pioneer-funded researchers, Web of Science author search results.](img/pf_wos_results.png){#fig-wos fig-env="figure*"}

For further consideration in this side of the corpus, we excluded *Mankind Quarterly* (as already included on the other side) as well as *Science* and *Nature* (as too general).  Three journals published by the American Psychological Association (APA; *American Psychologist*, *Contemporary Psychology*, and *Journal of Educational Psychology*) had to be excluded due to confusion over who could give us permission to use the archives for a text mining project, with both APA and ProQuest asserting that permission could only be granted by the other entity.  *European Journal of Personality*, published by SAGE, also had to be excluded because our institutional access only went back to 1999.  

The remaining 5 journals are all published by major academic publishers — Elsevier, Springer, or Cambridge University Press — and each item in the entire run of each journal has been assigned a DOI (digital object identifier) for archival purposes.  We used the Crossref API and `rcrossref` R interface to this API [@ChamberlainRcrossrefClientVarious2019 version 1.1.0.99] to retrieve metadata for each item published from 1960-2010 in each of these journals.  These metadata included item-level license information — confirming that the text of each item could be used for text mining projects — and a URL to an electronic version of the item.  These URLs were used to retrieve a XML or PDF version of each item, except for *Personality and Individual Differences*.  This journal has published a relatively large number of non-article documents, such as book reviews and commentaries, that are not available at the URL included in the Crossref metadata.  (This is unfortunate, as it was not difficult to find highly relevant documents that we could not automatically retrieve and therefore had to be excluded from our corpus.  One set of examples is an exchange between Rushton and Flynn on the Flynn effect: @FlynnEvidenceRushtonGenetic1998; @RushtonSecularGainsIQ1998; @FlynnReplyRushtonGang1998.)  Instead we used Elsevier's ScienceDirect API (<https://dev.elsevier.com/>) to independently search and retrieve all available items from *Personality and Individual Differences*.  This stage of corpus assembly was conducted between 2021-11 and 2022-05.  

*Behavioral and Brain Sciences* typically uses a target article + commentary format; in some cases there are dozens of commentaries for a single article.  In the Crossref DOI metadata, each target article and individual commentary is given its own DOI, with no distinction between contribution types and no metadata link between a commentary and the corresponding target article.  But text is only available in aggregate PDFs that bundle together the target article and all of the commentaries.  In somes cases this results in PDFs that are hundreds of pages long and might be linked from the Crossref metadata 30+ times.  In addition, the retrieved PDFs are not perfectly identical, because Cambridge UP's servers add a timestamped watermark to each page when the PDF is requested.  We contacted Cambridge UP for assistance but did not receive a response.  We ultimately used a series of ad hoc measures to mitigate text duplication.  Approximately 100,000 documents that were 1-2 pages long (per Crossref metadata) were excluded before PDF retrieval.  After PDF retrieval and text extraction, the timestamped watermarks were removed using a regular expression and the text was hashed using SHA 256.  Hashes were used to efficiently construct groups of identical documents, and a single document (whichever one happened to be first in the dataframe) was chosen from each hash group for inclusion in the corpus.  It is plausible that some duplicates made it through this process: where the watermark overlapped with the text, the regular expression likely would have been unable to identify and remove the watermark (a false negative result), and this small difference in the watermarks (not the text) would produce different hashes.  

After XML/PDF versions were retrieved, text was extracted using either the `xml2` package [@WickhamXml2ParseXML2023] or `pdftools`.  Figure @fig-counts and table @tbl-counts show the number of fulltext documents included in the corpus.  All together, the corpus includes 34,896 documents from 6 journals.  


## Text preparation 

After document retrieval and text extraction, we pre-processed the text using the `spaCy` NLP (natural language processing) Python library [@HonnibalSpaCyIndustrialstrengthNatural2018 version 1.9.0] and the R API `spacyr` [@BenoitSpacyrWrapperSpaCy2020 version 1.2.1].  Specifically, we applied regular expressions to remove header/footer copyright notices and hyphenation, used spaCy to annotate and extract noun phrases (eg, "the intelligence test items"), and then cleaned and standardized these phrases (eg, removing the/an/a, coverting all text to lowercase, and replacing all whitespace with underscores: "intelligence_test_items").  We then counted the occurrence of each noun phrase in the document.  The aggregated "document-term matrix" was stored in Parquet format [@VohraApacheParquet2016] for performance reasons, and written and read using the `arrow` package for R [@RichardsonArrowIntegrationApache2023 version 7.0.0].  

NLP-extracted noun phrases offer a number of advantages over the more traditional unigram ("single word") terms.  First, noun phrase extraction removes many standard stopwords (articles, common verbs, numbers) without relying on a fixed, a priori stopword list.  Phrases can be more informative than single terms, for example, distinguishing "intelligence test" from "hypothesis test."  While simple n-gram extraction will include numerous phrases that are not especially meaningful.  Consider the sentence "Since our first analyses of feeding patterns in rats, we had been using a criterion of 40 minutes (Le Magnen & Tallon 1963; 1966)" [@SclafaniCorrelationCausationStudy1981].  Bigrams such as "since our" and "criterion 40" will likely be discarded in vocabulary selection, but significantly increase the computational cost of vocabulary selection.  Noun phrase extraction is therefore more efficient.  For examples of previous research using noun phrases, see @HicksImpactingCapabilitiesConceptual2018; @HicksProductivityInterdisciplinaryImpacts2021. 

After noun phrase extraction, the corpus comprised 43,162,055 total tokens of 6,320,783 distinct phrases.  

### Data and code availability 

Document metadata retrieved from Crossref does not appear to be covered by any copyright or other intellectual property restrictions.  However, due to copyright restrictions, we are unable to make document fulltext or Web of Science search results publicly available.  Code for the corpus assembly and text preparation steps described above is available by request. 

The public analysis repository, <https://github.com/dhicks/race-science> *[DOI will go here]*, includes document metadata and documentwise counts of NLP-extracted noun phrases ("document-term matrices"), along with code to reproduce the analysis of the following sections. 


## Vocabulary selection 

We used a variation on the information-theoretic "`ndH`" approach to vocabulary selection from @HicksProductivityInterdisciplinaryImpacts2021.  In the current project, we found that that the uniform baseline distribution over documents in this approach heavily favored recurrent noun phrases in the longest documents.  Many of the documents published in *Psychological Reports* are extremely short, 1-2 page brief notices of a single study; while many of the documents published in *Brain and Behavioral Sciences* are book-length collections that include a long review article and sometimes dozens of commentaries.  Very generic noun phrases that happen to appear in BBS can occur orders of magnitude more often than highly distinctive phrases in PS, and so the term frequency $log(n)$ factor overwhelms the information gain $\Delta H$ factor.  

To address this, we used a different baseline distribution of documents, namely, one in which document probability is proportional to length.  This makes phrases from short documents much more "surprising" (much less likely to occur according to the baseline), and hence substantially increases their informativeness.  This was more effective at identifying useful phrases from across the corpus.  

A common rule of thumb in topic modeling is that the vocabulary should have about 10 times as many distinct terms as the number of documents in the corpus.  However, we had some concerns with computational demands here:  the resulting document-term matrix would have roughly $10 \times 33,000^2$ or 10.9 billion entries; with 10% density this would require on the order of 4 GB of memory just for a single copy of the matrix; and most of the analysis was to be conducted on the authors' laptops.  We therefore chose to work with three smaller vocabularies, $5 \times$, $1 \times$ and $\frac{1}{5} \times$ the number of documents.  We refer to these as the "large" (174,480 distinct phrases), "medium" (34,896) and "small" (6,979) vocabulary, respectively.  We compare findings across vocabularies as a robustness check.  


## Topic modeling 

We used topic models as a clustering (unsupervised machine learning) technique, to identify race science discourse within the corpus.  

To fit these topic models, we followed the approach proposed by @RoheVintageFactorAnalysis2020, which uses varimax-rotated partial principal components instead of the variational inference methods used by standard topic model packages such as `stm` [@RobertsStmPackageStructural2019].  This novel approach was implemented in the R package `tmfast` [@HicksTmfastFitsTopic2023].  We conducted a simulation study of `tmfast`, which found that it was significantly faster and only slightly less accurate at reconstructing known word-topic and topic-document distributions, compared to `stm` [@HicksTmfastFitsTopic2023].  We report a specific proof-of-concept simulation for the current study in @sec-poc. 

Topic models were fit for all three vocabularies (large, medium, and small) with $k = 5, 10, 20, ..., 70$ (number of topics), resulting in a total of $24 = 3 \times 8$ models. 

Following the approach of @HicksProductivityInterdisciplinaryImpacts2021, we did not attempt to identify a unique best fitted model for further analysis.  While the results section of this paper focuses on the medium vocabulary, $k=40$ model as the "median" among the 24 fitted models, qualitatively similar results appear across all models with $k \geq 10$ for all vocabularies; see @sec-topic-quality and figures @fig-grid-lg, @fig-grid-md, @fig-grid-sm.  

To analyze the fitted topic models, we first constructed and manually inspected visualizations of the topic-document distributions (see figures @fig-gamma-lg, @fig-gamma-md, @fig-gamma-sm) and the top (highest-probability) terms for each topic (see figures @fig-silge-lg, @fig-silge-md, @fig-silge-sm).  This suggested that the models often identified two distinct topics with racial terms, that some of these topics used terms from intelligence research, and that these topics had different occurrence patterns across journals.  To examine these patterns more systematically, we programmatically identified every topic across all 24 models that used some combination of racial and intelligence terms, and assembled visualization grids of the occurrence of these topics by journal over time (see figures @fig-grid-lg, @fig-grid-md, @fig-grid-sm).  

This analysis relied on an interpretive assumption that topics with racial top terms were reliable indicators of race science discourse.  To confirm this assumption, we conducted two quality assessments of topic 24 from the medium vocabulary, $k = 40$ model.  A manual assessment by both authors achieved consensus that 89% (108/121) of the top (highest likelihood) papers in this topic were race science discourse; see @sec-topic-quality.  We also replicated the medium vocabulary, $k = 40$ model using the popular `STM` package for topic modeling; see @sec-stm.  This model also gave qualitatively similar results.  Due to the time and computational resources required for these interpretive checks, we determined that it would not be feasible for us to repeat them across all 855 topics ($3 \times (5 + 10 + 20 + \cdots + 70)$) and 24 topic models.  

## Limitations 

With respect to the aim of detecting and studying the prevalence of race science, a critical limitation of our methods is that they cannot distinguish race science from the broader category of race science discourse. Topic modeling represents a document as an unstructured "bag of words"; while it can detect that the document discusses both intelligence and racial groups, for example, it cannot detect whether the author endorses or critiques race-and-intelligence research. For the same reason, topic modeling cannot determine whether an article purports — implicitly or explicitly — to justify racial inequality and colonialism by appealing to scientific (or pseudo-scientific) findings, and thus it cannot distinguish race science discourse from scientific racism.  We therefore can only make claims about finding race science discourse, except when manual inspection or prior research indicates otherwise [e.g., @JacksonJr.ArthurJensenEvolutionary2022].  

To generalize this limitation, the text mining techniques we use here are unable to draw on a broader cultural context than what is represented in the corpus.  Scholars of racism and US race relations have noted a shift in the way racist attitudes are understood and expressed, from overt and direct to a subtle "color-blind" racism [@Bonilla-SilvaRacismRacistsColorBlind2006]. Color-blind racism represents a change in the primary mechanisms by which white privilege is maintained, using non-racialized language in furtherance of the racial status quo both institutionally and individually, and increasingly reliant on the invisibility of socio-cultural mechanisms to preserve inequalities. Color-blind racism involves the use of racialized "codes" or "dogwhistles," language that is superficially non-racial but carries racial implicatures, such as (in the US context) "violent inner-city criminals" or "welfare queen" [@SaulDogwhistlesPoliticalManipulation2018].  Alderfer notes that the racist implications of *The Bell Curve* only emerge gradually, with the opening of the book using the color-blind language of "group differences" [@AlderferScienceNonsciencePsychologists2003].  These cases suggest further that color-blindness can facilitate white supremacist appropriation of non-racial research — even contrary to the researchers' own intentions — as scientific racist readers re-interpret racial-neutral language as dogwhistles [@GillbornSoftlySoftlyGenetics2016; @WillsAreClustersRaces2017].  

The phenomenon of color-blind racism is closely related to ambiguities that cannot be resolved even by human coders familiar with the broader cultural context.  Two relevant examples are the "Flynn effect," a secular increase in intelligence test scores noted by philosopher James Flynn; and studies of "national IQ," exemplified by the work of Richard Lynn.  The Flynn effect implies that environmental factors can create group differences in IQ that are comparable to Black-White differences, and thus directly challenges claims that the racial differences are biological.  Using methods widely regarded as ad hoc, cherry-picking, and generally unreliable, Lynn and collaborators have claimed that national IQ averages are correlated with national GDP [@SamorodnitskyJournalsThatPublished2024].  Both the Flynn effect and "national IQ" are strongly associated with race-and-intelligence discourse in the US cultural context.  But scientific journal articles on these topics may not include any racial language at all.  For example, a meta-analysis of the Flynn effect does not discuss racial differences or use terms such as "White" or "Black" outside the reference list [@TrahanFlynnEffectMetaanalysis2014].  Should such articles be coded as race science discourse?  We expect that even readers who accept our definitions of race science and race science discourse will reasonably disagree on how to answer this question.  Topic models, of course, are not even capable of representing this essential ambiguity.  

Recently, some computational scholars have argued that foundational large language models (LLMs) are able to encode broad cultural contexts [@UnderwoodWhyAINeeds2024] and produce sophisticated analyses of arguments [@CaulfieldTwoMeaningsReasoning2024]. If they are right, future LLM-based methods might possibly be able to distinguish supporters from critics of race-and-intelligence research, detect the use of racial dogwhistles, and perhaps even reliably identify the fuzzy boundaries of essentially contestable categories such as race science. However, such methods are beyond the scope of the project we present here. 


# Results

## *Mankind Quarterly* and Pioneer-funded researchers 

We identified 16 researchers who had received funding from Pioneer; 14 of these researchers had profiles in the Web of Science (WoS) author search, allowing us to identify 13 WoS-indexed journals that had published 6 or more of these authors.  See table @tbl-researchers and figure @fig-wos. As the affiliations in table @tbl-researchers indicate, almost all of these researchers were faculty at major research universities.  

Figure @fig-wos shows that, while MQ is among the "Pioneer-publishing" journals, a number of mainstream journals are more prominent: *Personality and Individual Differences* (PID), *Intelligence* (Int), *Behavior Genetics* (BG), and *Psychological Reports* (PR).  In addition, only psychologist Richard Lynn appears to have published heavily in MQ.  Lynn became an assistant editor of MQ in 1979 (vol. XX, Nos. 1 & 2) and is listed as editor-in-chief on MQ's current website as of 2023-07-21.  He was also president of Pioneer from the death of psychologist J. Phillippe Rushton in 2012 [@BeirichPioneerFundAssets2013] until his own death in 2023.  By contrast, a number of Pioneer-funded researchers have published a significant number of papers in PID, Int, and to a lesser degree BG: Bouchard, Eysenck, Jensen, Rushton, Vernon, and also Lynn.  

*Personality and Individual Differences* (PID) was founded in 1980, with Eysenck as editor-in-chief and an editorial board including Jensen and Lynn. In the inaugural editorial, Eysenck identified "studies of the genetic determinants of individual differences in the areas of personality and intelligence" as one of the journal's eight major areas of interest.  Eysenck remained editor-in-chief until his death in 1997.  In 2005 the editorial board still included Jensen and Lynn. PID was first published by Pergamon Press, a mainstream academic press, and today is published by Elsevier.  

*Intelligence* (Int) was founded in 1977, with psychologist Douglas Detterman as editor-in-chief from the founding until 2016.  Lloyd Humphreys was on the editorial board starting from 1977; by 1990 he had been joined by Jensen and Philip Vernon.  Richard Lynn joined the editorial board sometime between 1998 and 2002.  (Archive copies of the Int editorial board page are not available from the journal's website from 1999 through 2001.)  Int has been criticized for including Lynn and Gerhard Meisenberg — who was editor-in-chief of MQ in 2015-18 — on its editorial board until 2018 [@SainiSuperiorReturnRace2019].  Int is published by Elsevier.  


## Topic model analysis identifies race science discourse 

Per our hypothesis that MQ served as an "incubator" or "safe space" for race science, we had expected the topic models to identify a distinct race science discourse topic, that this topic would appear first and most prominently in MQ, and only later "spread" to mainstream journals.  Instead, the models robustly identified two distinct kinds of race science discourse.  One of these was almost exclusive to MQ; the other originated outside of MQ and was almost always more prevalent in mainstream journals, especially Int and PID.  

![Silge plots [@SilgeTopicModeling2017] and smoothed time series for three focal topics from the medium vocabulary, $k = 40$ model.  Topics 07 and 24 are examples of the two distinct types of race science discourse, while topic 22 is "mainstream" or non-race science intelligence research.  Top row: top 15 terms by $\beta$ (term-topic distribution) for each topic; text version in table @tbl-silge.  Bottom row: count of articles associated with each topic, by journal and year.  Article counts use a threshold approach, with $\gamma$ (topic-document distribution) greater than 0.5.  Thin lines give annual values, thick lines give 5-year running averages.  Because *Behavior Genetics* and *Behavioral and Brain Sciences* are not prominent in any panel, neither is given a direct label.](img/08_focal_topics.png){#fig-focal}

Figure @fig-focal focuses on three topics identified in one of the 24 fitted topic models; see table @tbl-silge for a table version of the Silge plot in @fig-focal; figures [@fig-gamma-lg; @fig-gamma-md; @fig-gamma-sm; @fig-silge-lg; @fig-silge-md; @fig-silge-sm] for all topics from all models, and figures [@fig-grid-lg; @fig-grid-md; @fig-grid-sm] for selected topics from all models; we discuss the robustness of our findings across all models below.  In the focal model, topic 07 is strongly associated with MQ:  MQ published dozens of articles in this topic each year, and no other journal ever published more than a handful.  The "Silge plot," showing the top 15 terms in the topic [@SilgeTopicModeling2017], contains a number of racial terms (`races`, `whites`, `negroes` and potentially `europe`, `africa`, `india`, and `japan`); as well as `book`, likely reflecting the fact that MQ published a number of book reviews, while the mainstream journals either did not publish book reviews or (in the case of PID) these were not available for the corpus. 

Topic 22 is strongly associated with Int and PID in the same way that 07 is associated with MQ.  The Silge plot does contain `jensen`, as well as a reference to Raymond Cattell, who played a major role in the development of factor analysis and intelligence testing but also advocated for eugenics, fascism, and Nazi race science [@MehlerBeyondismRaymondCattell1997]. However, the other authors named in this topic — John Horn and Peter Bentler — do not appear to have contributed to race science.  Instead this topic appears to identify "mainstream" (non-race science discourse) intelligence research, especially factor analysis and the debate over whether intelligence is unidimensional or multidimensional.  (Three other topics only associated with mainstream intelligence journals and terms were also identified by this topic model.)  This topic indicates that the model is able to distinguish "mainstream" intelligence research from race science discourse.  

The Silge plot for topic 24, by contrast, suggests a distinct race science discourse topic, with multiple racial terms (`whites`, `blacks`, `racial_differences`, `races`, `race_differences`) and the names of three prominent Pioneer-funded researchers, `jensen`, `lynn`, and `rushton`.  Independent qualitative coding of 121 papers in this topic (those with $\gamma > 0.97$ and published in journals other than PR) confirmed this interpretation, with 108 (89%) coded as race science discourse by both authors (Cohen's $\kappa = 0.86$); see @sec-topic-quality.  

In almost all years, most papers in topic 24 were published in mainstream journals rather than MQ.  Jensen's "How Much Can We Boost IQ and Scholastic Achievement?" [@JensenHowMuchCan1969] was published in 1969 in *Harvard Educational Review* (not included in this study), and the time series indicates that, in the early 1970s, there was an increase in articles in topic 24 in both PR and MQ (the only two journals in our corpus that were active at the time).  MQ shows another sharp increase in the late 1980s; using content analysis, Adams and Pilloud found that psychology was the dominant discipline in MQ in the period 1992-2018 [@AdamsMisappropriationBiologicalAnthropology2021].  PID published multiple papers in topic 24 almost immediately after it was founded, with Int showing a more gradual increase between the mid-1970s and mid-1990s.  

These qualitative results are robust across models using the large and medium vocabularies; see @fig-grid-lg, @fig-grid-md, @fig-grid-sm.  Across all vocabularies, as $k$ increases, multiple different "intelligence only" topics emerge, apparently picking out different methods or aspects of intelligence research.  For example, there is often an "intelligence only" topic that is highly associated with BG.  MQ never has many publications in an "intelligence only" topic, except for $k=10, 20$ in the large vocabulary; in these two models, there is neither a "race only" nor "race-and-intelligence" topic.  

In the large and medium vocabularies, there is usually one "race only" topic and/or one "race-and-intelligence" topic, and the "race only" topic is always virtually exclusive to MQ.  In the medium vocabulary, "race-and-intelligence" is more prevalent in mainstream journals than in MQ; except for $k=20$, where there is no "race only" topic.  In the large vocabulary, a "race only" topic does not emerge until $k=40$, and there are not simultaneous "race only" and "race-and-intelligence" topics until $k=60$.  However, in $k=60, 70$ there are both topics, and they follow the same pattern seen in the medium vocabulary: "race only" is almost exclusive to MQ, and "race-and-intelligence" is more prevalent in mainstream journals. 

Unlike the other two vocabularies, in the small vocabulary there are two "race only" topics for $k \geq 50$.  Across values of $k$, topic V04 is more prevalent in PS through about 1990; the other "race only" topic is typically more prevalent in MQ, but also includes a few articles from PS. For $k \geq 40$, the small vocabulary models have distinct "race only," "race-and-intelligence," and "intelligence only" topics; "race-and-intelligence" is always predominantly mainstream, and increases in prevalence over the course of the 1980s.  The "extra" "race only" topic appars to fade out quickly over the course of the 1980s, and its top terms include `race differences`, `rushton`, and `crime`. So one possibility is that that the small vocabulary is able to pick out a distinct line of race science discourse that is focused on crime and/or other behaviors, rather than intelligence, and was replaced with a more intelligence-focused line, perhaps in a way that anticipated publication of *The Bell Curve* in 1994.  However, we think it's more plausible that this separation is an artifact due to the very small size of this vocabulary, with many fewer terms than documents. 

## Race science discourse and behavior genetics

BG published very few articles in any race-and-intelligence topic identified by any of the 24 topic models.  But, among both (other) academics and the general public, the field of behavior genetics is strongly associated with race science, and specifically race-and-intelligence research. @PanofskyMisbehavingScienceControversy2014 argues that, prior to Jensen's 1969 paper, behavior genetics emphasized the study of non-human animals and intentionally avoided associations with eugenics and public controversy more generally. In response to Jensen, critics such as biologist Richard Lewontin offered broad critiques of behavior genetics as such, and behavior geneticists in turn adopted a radical conception of academic freedom (namely, one that denies any sense of responsibility for the social implications of academic research) along with a siege or wartime mentality, as illustrated by Sandra Scarr's 1986 presidential address to the Behavior Genetics Association (BGA) [@ScarrThreeCheersBehavior1987].  Scarr's address coincided with the period between 1970-1990 when BG published articles (including Scarr's address itself) in the race science discourse topic (24) of the medium vocabulary, $k = 40$ model. 

However, the topic model analysis suggests that, by the 1990s, behavior genetics may have distanced itself somewhat from race science, albeit without directly repudiating it. This interpretation was supported by a supplemental analysis that focused on BG specifically; see @sec-bg.  For example, the 1995 BGA presidential address by Glayde Whitney --- in which he criticized the "Marxist-Lysenkoist denial of genetics" and proposed that differences in murder rates between countries and cities were caused by racial genetic differences in intelligence, empathy, aggression, and impulsivity --- was published in MQ rather than BG. Behavior genetics may have been prominent in promoting race science in the past, but appears to have been less receptive to such ideas in more recent history.  Instead, our results suggest that other subdisciplines of psychology — perhaps especially cognitive psychology — have provided the primary venue for mainstreaming race science.  

(Late in the publication cycle, it occurred to us that evolutionary psychology is often associated with race science in the same way as behavior genetics. Because no evolutionary psychology journals were included in the corpus, we are unable to examine this association here. However, this would be a worthwhile topic for future research.)


## Disproportionate presence of Pioneer-funded authors in race science discourse

![Disproportionate presence of Pioneer-funded authors in three focal topics.\
(A) Number of authors for three focal topics from the medium vocabulary, k= 40 model, by year. Mainstream intelligence (topic 22) shows exponential-like increase while MQ race science (topic 7) and race-and-intelligence research (topic 24) remain flat. \
(B) Pioneer-funded authors as a share (%) of all authors publishing in a given topic, by year (thin lines) and 5-year running mean (thick lines). Usually less than 15% of authors in a topic are Pioneer-funded. \
(C) Share of papers (%) with at least one Pioneer-funded author, by year (thin lines) and 5-year running mean (thick lines). Not shown is 1963 for topic 22, where Robert A. Gordon was one of two coauthors of the single publication in that topic that year. After about 1985, more than 15% of papers in topic 24 have at least one Pioneer-funded author. \
(D) Comparison of author share (panel B) to paper share (panel C). Each point is one year-topic combination. Not shown here is 1963 for topic 22. Points above the dark line $y = x$ indicate Pioneer-funded authors had a greater paper share than would be expected based on author share. Length of thin vertical lines measures "presence." \
(E) Empirical density plots of "presence," mean difference between paper share and author share for 1,000 randomly drawn sets of 13 authors. Bold vertical lines are the mean values for the Pioneer-funded authors (signed length of lines in panel D). Pioneer-funded authors are similar to randomly drawn sets for topics 7 and 22, but very different for topic 24. ](img/15_presence.png){c}

Post hoc, we were interested in trying to understand the impact that Pioneer might have had as an institution for supporting friendly scientists. (We thank a reviewer for comments that suggested this further analysis.) Because we did not have citation data, we were unable to examine "impact" in its usual sense of citations. Instead we chose to examine "presence," which we operationalized as the difference between the share of papers with at least one Pioneer-funded author ("paper share") against the share of all authors who were on our list of Pioneer-funded authors ("author share"). Positive values of "presence" would indicate that Pioneer-funded authors publish more (in a given topic) than might be expected. 

Because this was a post hoc analysis, we elected to examine "presence" only for the three focal topics for the medium vocabulary, $k=40$ model (figure @fig-focal). Figure @fig-presence shows the results of this analysis. Details of the methods used here are reported in @sec-presence. Note that papers were assigned to topics in a different, less conservative way than in @fig-focal. 

Subfigure @fig-presence A shows that topics 7 (MQ-style race science) and 24 (race-and-intelligence) had stable numbers of authors over the scope of the study, while topic 22 (mainstream intelligence) saw an exponential-like increase. Subfigure @fig-presence B indicates that Pioneer-funded authors were a relatively high share of authors in topic 24, often comprising more than 10% of all authors in this topic after 1980. In contrast, these authors were almost never more than 10% of all authors in the other two topics.  

Subfigure @fig-presence C shows that a greater share of papers in topic 24 had Pioneer-funded authors, compared to the other two topics, throughout the scope of the study but especially after about 1980. 

Subfigure @fig-presence D is a scatterplot of paper share (subfigure C) against author share (subfigure B), by year and topic. The dark diagonal line $y = x$ corresponds to equality between paper share and author share. Distance above (below) this line indicates that Pioneer-funded authors had disproportionately greater (less) "presence" than would be expected based on their authorship profile. For topics 7 (MQ-style race science) and 22 (mainstream intelligence), Pioneer-funded authors consistently had a small negative to modestly positive presence; with the exception of 1963 (not shown on this plot), where Robert A. Gordon was one of two coauthors (50% author share) of the sole paper in topic 22 that year (100% author share).  In contrast, for topic 24 (race-and-intelligence), Pioneer-funded authors consistently had a substantial, disproportionately positive presence, with in some cases differences between paper share and author share of 20 points or more. 

Subfigure @fig-presence E uses a resampling method to compare the observed value of presence for the Pioneer-funded authors (dark vertical lines) to a "random" set of 13 other authors publishing in the topic (density curves and rugs). Compared to other authors, the Pioneer-funded authors do not have a distinctive presence for topic 7, are somewhat greater than the mode for topic 22, and are at the rightmost extreme for topic 24. 

All together, this post hoc analysis suggests that the Pioneer Fund had a disproportionate presence in race-and-intelligence research, but not in MQ-style race science or mainstream intelligence research. 





# Discussion 

Using scientometric and text-mining methods, this study finds that Pioneer-funded researchers published heavily in certain mainstream psychology journals, much more than in *Mankind Quarterly* (MQ); that a distinct version of race science discourse, centered on race and intelligence, can be identified in these same mainstream journals across the period 1960-2010; and that authors funded by the Pioneer Fund had a disproportionate presence in this latter race science discourse, and not in the variety of race science discourse published by MQ.  These findings indicate that MQ was a less important venue for late 20th century race science discourse than mainstream psychology journals — especially *Intelligence* (Int) and *Personality and Individual Differences* (PID).  Indeed, until the 1990s, MQ published a very different kind of race science discourse from the race-and-intelligence research published in Int and PID.  These results contrast with much of the qualitative historiography of scientific racism, which has often emphasized MQ [@MehlerFoundationFascismNew1989; @WinstonScienceServiceFar1998; @TuckerFundingScientificRacism2002; @SchafferScientificRacismAgain2007; @WinstonScientificRacismNorth2020; @SainiDraperMillionsPhilanthropic2022; @AdamsMisappropriationBiologicalAnthropology2021 also emphasize MQ, but in relation to anthropology rather than psychology]. 

During this same period of time, race scientists served in prominent and influential positions in behavior genetics and psychology, including as society presidents and members of journal editorial boards.  In some cases, some race scientists simultaneously maintained active connections to both the mainstream scientific community and white supremacist organizations [@WinstonScientificRacismNorth2020; @JacksonJr.ArthurJensenEvolutionary2022; @SlobodianUnequalMindHow2023].  Thus the emphasis on MQ in race science historiography may have been misplaced. At the same time, our findings indicate that the Pioneer Fund may have played an important role in promoting race-and-intelligence research in mainstream psychology. 


## Agnogenesis within the scientific community

In terms of the margins-mainstream-bridge model of the "echo chamber effect," there was no bridge that carried ideas originating in MQ on the margins to mainstream journals, because the kind of race science discourse that originated in MQ did not spread to the mainstream. At the same time, within the scope of our data, the race-and-intelligence version of race science discourse was always a mainstream phenomenon, and as a mainstream phenomenon was potentially supported by the Pioneer Fund. All together, the Pioneer Fund, *Intelligence*, and *Personality and Individual Differences* appear to fit with the strategy of promoting friendly research, rather than the echo chamber strategy [@FernandezPintoKnowBetterNot2017]. 

The circulation of ideas is a prominent theme in the agnotology literature.  Researchers have often examined the public circulation of ideas — how an industry attempts to manage public concerns about its products in marketing, press releases, and interviews — as well as the circulation of ideas within a company or industry itself — as in memos among intramural scientists and management [@ProctorGoldenHolocaustOrigins2012].  In a series of articles, Supran and Oreskes have shown that Exxon/ExxonMobil lied about climate change — knowingly made false statements — by contrasting skeptical and denialist statements in public-facing documents with skillful climate projections by the company's own scientists [@SupranAssessingExxonMobilsClimate2017; @SupranAddendumAssessingExxonMobils2020; @SupranRhetoricFrameAnalysis2021; @SupranAssessingExxonMobilsGlobal2023]. @DasguptaBiologicalEssentialismWhite2024 and @DasguptaRacistAgnotologyHow2025 examine how white nationalists deploy agnotology-like analytical frames to critique and appropriate mainstream racial disparities research — research often explicitly motivated by an anti-racist agenda. 

However, empirical research in agnotology has paid less attention to the circulation of ideas within the scientific community.  @SupranAssessingExxonMobilsGlobal2023 include several peer-reviewed journal articles in their content analysis; but compare them to intramural documents and public statements, rather than the broader scientific literature.  @FernandezPintoKnowBetterNot2017 gives three examples to illustrate the "tobacco strategy": the joint public relations-research funding campaign by the tobacco industry itself; medical ghostwriting by the pharmaceutical industry; and efforts by conservative think tanks to promote a simplistic account of the causes of the Great Recession that shifted blame away from deregulation of the finance industry.  Both the first and third cases emphasize the circulation of ideas to the public, and the third does not involve academic researchers in any significant way.  Medical ghostwriting is indeed concerning as a strategy for circulating false or misleading claims in the scientific literature. But it has not been a major case in the agnotology literature in recent years. This might reflect less concern about ghostwriting in biomedical research over time: on 2025-01-16, a Web of Science search for `medic* ghostwrit*` returned 85 total papers published during 2003-2024, with the number of publications per year peaking at 11 in both 2010 and 2011 and declining to 1-2 per year in 2023 and 2024. 

Some researchers have used agent-based models (computer simulation methods) to study the circulation of misinformation within scientific communities under agnogenic ("ignorance-creating") influences such as "industrial selection" and increased dissemination of favorable views [@HolmanExperimentationIndustrialSelection2017; @WeatherallHowBeatScience2018; @OConnorMisinformationAgeHow2019]. Combined with our findings that Pioneer-funded authors had a disproportionate presence in race-and-intelligence research, these models suggest that the Pioneer Fund might have "maintained the controversy" within the scientific community by supporting friendly scientists. However, while these models can give insight into how certain mechanisms might work, it is difficult to link them to empirical data [@FernandezPintoEpistemicLandscapesReloaded2018; @HicksOpenScienceReplication2023]. 

Furthermore, doubt or ignorance about race within the community of psychological science is maintained not only by the Pioneer Fund and its small group of "friendly scientists" [@WinstonWhyMainstreamResearch2020] but also by the ordinary norms and standards of the broader scientific community. @roberts2023challenging analyzed over 1700 articles published between 1995 and 2018 in seven psychology journals, finding that research using White samples was more likely to be presented in race-neutral language than research using samples of people of color. @roberts2023challenging further argued that psychology as a discipline treats White as a default and "nonracial" description of humanity. 

Understanding the public circulation of race science — its translation into scientific racism — is essential. But our findings suggest the circulation of race science within the scientific community warrants further investigation.  Using the set of race science discourse articles identified in this project as a starting point, future research might examine patterns of citations or — perhaps by applying large language models (LLMs) — the appearance of certain modes of argument or types of evidence.  For example, to what extent have race science discourse articles been cited within non-race science articles?  Do race science discourse articles appeal to distinctive kinds of evidence, or deploy distinctive modes of interpretation? 



## Moral responsibilities of cognitive psychology

We are not identifying any particular scientist as a racist, scientific racist, or race scientist based on the topic model results alone.  Such claims require an analysis of documentary evidence that goes beyond the scope of the current study [@JacksonJr.ArthurJensenEvolutionary2022].  We do assume that our readers, like us, regard white supremacy and scientific racism as morally odious and beyond the scope of reasonable debate [@SchroederLimitsDemocratizingScience2022].  However, scientific research can be appropriated to promote scientific racism — and thus count as race science — even when this is contrary to the intentions of the original researchers themselves [@TaberyWhyStudyingGenetics2015; @GillbornSoftlySoftlyGenetics2016; @CarlsonQuantifyingContextualizingImpact2020; @HennWhyDNANo2021].  Panofsky et al. show how the term "human biodiversity," originally developed for anti-racist purposes by biological anthropologist Jonathan Marks, has been inverted by white supremacists and is now a dogwhistle for biological racial hierarchy [@PanofskyHowWhiteNationalists2021].  

A natural question, then, is what responsibilities does cognitive psychology have to mitigate the field’s contributions to race science and scientific racism? 

Over the past few decades, many professional organizations in genetics and anthropology have made formal statements rejecting race as a biologically meaningful concept [@anthropology1996aapa; @american2018ashg; @board1998aaa; @committee2020asa; @FuentesAAPAStatementRace2019; @nhgri2023; @rotimi2022ashg; @wynshawboris2020ashg], undermining the core assumption of race science.  The American Psychological Association (APA) --- whose members are predominantly clinical psychologists --- the Federation of Associations in Behavioral & Brain Sciences (FABBS) --- an organization whose purpose is to provide policy recommendations --- and the editors of *Nature Human Behavior* have openly rejected race as biologically meaningful [@american2021apology; @baron2022fabbs; @ScienceMustRespect2022].  But research-focused psychological organizations like the Association for Psychological Science (APS) and Psychonomic Society (PS) stop short of rejecting race as biologically meaningful when they denounce racism [@aps2021statement; @board2020psychonomic].  Psychology as a discipline maintains space for several mythological race science narratives, such as allegations of a taboo against race and intelligence research [@jackson2021mythical], claims that scientists engaging in or calling for any kind of anti-racism in the field is an ideological corruption of dispassionate and value-neutral science [@roberts_2022], or arguments that holding race science to the same evidentiary standards as other psychological research is a violation of academic freedom [@herbert2023academic].  And historically several race scientists have secured important gatekeeping positions within the scientific community, such as on the editorial boards of *Intelligence* and *Personality and Individual Differences*.  

Research on the genetics of intelligence has been socially harmful, not just in the somewhat abstract sense of promoting racial stigma, but in the concrete sense of being used to rationalize mass shootings and other acts of racial violence [@PronczukRacistResearcherExposed2022; @MeyerWrestlingSocialBehavioral2023].  While scientists do not fully control the downstream social effects of their research, like other citizens scientists are responsible for mitigating reasonably foreseeable harms that result from their actions [@BlockIQHeritabilityInequality1974a; @DouglasSciencePolicyValuefree2009; @KitcherArgumentFreeInquiry1997; @KouranyShouldKnowledgeBe2016].  These actions include not only individual decisions to research this topic or that, but also collective decisions about the allocation of scarce resources such as journal space, research funding, and positions of power and influence.  Given the way research on intelligence and behavior genetics has been used historically, these fields may be especially susceptible to appropriation by scientific racists.  This only strengthens the obligations of researchers in these areas to prevent their research from causing harm [@CarlsonQuantifyingContextualizingImpact2020; @OgbunugaforDNABasketballBirthday2022].  

<!-- Furthermore, the maintenance of doubt or ignorance about race within psychological science can be achieved not just via the explicit race science research of a small minority of psychologists who seek to create that doubt or ignorance in the first place [cf. @WinstonWhyMainstreamResearch2020], but also through the norms and traditions of the discipline that are engaged in by the average psychological scientist. For example, Roberts and Mortensen analyzed over 1700 articles published between 1995 and 2018 in seven psychology journals, finding that research using White samples was more likely than research using samples of color to be presented in race-neutral language [@roberts2023challenging]. They reported this finding in the context of a larger argument about how psychology as a discipline treats White as a default and nonracial description of humanity and offering suggestions for how psychology can move past this longstanding implicit yet discipline-wide framework. The treatment of Whiteness as racially neutral by psychology can, like the reluctance of psychological science organizations to reject racial categories as biologically meaningful, further the goals of those who wish to create racialized ignorance, and it is the responsibility of psychological scientists to rebuke efforts that create ignorance. -->

How should psychologists exercise these responsibilities?  We believe restorative or reparative justice provides a useful model [@WenzelRetributiveRestorativeJustice2008; @WormerRestorativeJustice2013].  Unlike retributive justice (used by almost all criminal justice systems today), the aim of restorative justice is to repair the damaged relationships between victims and perpetrators of injustice.  Typically, restorative justice requires perpetrators to not only acknowledge their actions and the harms that these actions had on victims, but also to work with victims to identify concrete actions that perpetrators can take to redress or mitigate these harms.  From this perspective, explicit apologies for contributions to scientific racism are essential as a first step [@FuentesAAPAStatementRace2019; @american2021apology; @JacksonFacingOurHistory2023], but not fully sufficient on their own.  



# Acknowledgments 

Thanks to Emily Merchant and John Jackson for some initial discussions that helped clarify the scope of the project and identify some key resources related to the Pioneer Fund and *Mankind Quarterly*.  Thanks to Anthony Sainez for help retrieving the *Mankind Quarterly* articles.  Thanks to Derek Devnich and James Dooley for their work attempting to secure the APA-published articles.  Thanks to Kevin Bird, John Jackson, and Manuela Fernández Pinto for comments on a draft of this paper. 

**Funding:** EJCL's work on this project was partially supported by UC Merced.  DJH's work on this project was not supported by any specific funding. 

**Author contributions:** DJH (conceptualization, data curation, formal analysis, investigation, methodology, project administration, software, validation, visualization, writing-original draft, writing-review & editing); EJCL (conceptualization, methodology, software, validation, visualization, writing-original draft, writing-review & editing)

**Competing interests:** The authors have no competing interests to declare. 

**Data and materials availability:** Web of Science search results and article fulltext are not publicly available due to intellectual property restrictions.  Preprocessed analysis data (document-term counts and article metadata) and analysis scripts are available at *[doi]*. 



# References 

::: {#refs}
:::


\appendix
\clearpage
\pagenumbering{arabic}
\renewcommand*{\thepage}{S\arabic{page}}
\renewcommand\thefigure{S\arabic{figure}}    
\setcounter{figure}{0} 
\renewcommand\thetable{S\arabic{table}}    
\setcounter{table}{0}
\renewcommand*{\thesection}{S\arabic{section}}


Supporting Information for "Race Science in Mainstream Psychology, 1960-2010"

D.J. Hicks, Emilio J.C. Lobato

\clearpage

![Corpus assembly.](img/dataset.png){#fig-corpus width="120%" fig-env="figure*"}

![Count of documents in the corpus, by journal and year.](img/02_count.png){#fig-counts  fig-env="figure*"}

Table: Document counts, by journal, and years included in the corpus. {#tbl-counts}

|journal                                |     n| start|  end|
|:--------------------------------------|-----:|-----:|----:|
|Behavior Genetics                      |  2268|  1970| 2010|
|Intelligence                           |  1237|  1977| 2010|
|Mankind Quarterly                      |  1821|  1960| 2004|
|Personality and Individual Differences |  7274|  1980| 2010|
|Psychological Reports                  | 21398|  1960| 2010|
|Behavioral and Brain Sciences          |   898|  1978| 2010|

```{r}
#| tbl-cap: "Top 15 terms, by beta value, from selected topics of the medium vocabulary, $k = 40$ model"
#| label: tbl-silge
#| echo: false
library(gt)

readRDS(file.path('..', 'out', '08_silge_gt.Rds')) |>
    # cols_width(everything() ~ pct(100/6)) |>
    tab_options(table.font.size = 10)
```

# Operationalizing "race science" in practice {#sec-operationalizing}

A reviewer raised a concern that our conception of race science involved a counterfactual that "could be difficult to confirm or refute." A full discussion of how, for example, a research funder might operationalize race science in order to subject it to heightened critical scrutiny before funding, is beyond the scope of the current project. Here we sketch three frameworks that might be used to operationalize race science, either independently or in combination. 

First, some research has actually been utilized in actual scientific racist propaganda, for example, in memes that circulate on fringe discussion boards or as citations in the manifestos of mass shooters. This research is ipso facto race science. Taken on its own, this framework is only useful after the fact. But it might still play a substantial role in, for example, a decision whether to retract a publication or adjudicate whether a researcher has engaged in misconduct. 

This first framework can also provide important evidence for the second framework, which draws on the *reasonable person* standard from the law of torts. In other words, the second framework asks whether a reasonable scientific peer — well-informed about what kinds of research have been utilized by scientific racists and how, and concerned to avoid this happening in the future — would consider the research likely to be utilized in a similar way in the future. @DouglasSciencePolicyValuefree2009 argues that, because we all have a moral responsibility to exercise reasonable foresight over our activities, scientists have a specific responsibility to exercise such foresight over the social consequences of their research. Reasonable foresight does require projecting counterfactual scenarios; but these can be based in casuistical reasoning (comparisons with actual cases, as per the first framework) and are not different in kind from the projections made by research funders and hiring committees.  

@PenceChallengesCommunityScience2023 argue that a reasonable foresight framework encounters difficulties as scientific communities become more nebulous and vaguely defined, and propose that exercising reasonable foresight should include community engagement and incorporating outsider perspectives. So, specifically, cognitive psychologists might make a point of engaging with groups that have been marginalized by race-and-intelligence research and working with scholars who have studied how and why certain kinds of research are taken up by scientific racists.  

The third framework starts with exactly this question of how and why research is taken up by scientific racists. In other words, what features of a project make it race science? 

Schematically, a research project can be thought of as using *methods* to produce *findings* that (aim to) answer *research questions*. Each of these three elements requires various assumptions to be logically relevant to the other two  [@LonginoScienceSocialKnowledge1990; @LloydAdaptationismLogicResearch2015]. For example, a particular project might use data from a national genome bank and a genome-wide association study (GWAS; method) to develop a polygenic score estimator of educational attainment (finding) with the aim of answering the question "To what extent is cognitive ability genetic?" This project must assume that a sample from a single nation is representative of humanity in general (to be relevant to the generalized framing of the question), that "cognitive ability" is a well-defined construct and worth investigating, that educational attainment is an accurate measure of cognitive ability, that the SNPs included in the genome data capture genetically-caused variation, that analysis of the polygenic scores can recover/measure this variation, that variation can be partitioned into genetic and environmental causes [@KellerMirageSpaceNature2010; @TaberyStruggleUnderstandInteraction2014], and so on. 

Some of these assumptions — or the findings themselves — can support scientific racist claims, either through strict deductive entailment (the kind studied in formal logic courses) or through *implicature* [@DavisImplicature2024]. The concept of "implicature" refers to the phenomenon that language use goes beyond strict deductive entailments — "we mean more than we say." Consider the following exchange between a teenager and their parent. 

| 
| Teen: Sasha's going to Cold Stone. 
| Parent: The dishes are still in the sink. 
| Teen: I don't have practice in the morning. 
| Parent: Be home by 10. 

Taken strictly, the exchange is nonsensical: Sasha going to Cold Stone has nothing to do with the dishes, which in turn has nothing to do with whether the teen has practice or what time they should be home. 

Instead the exchange runs on implicature. The teen's first statement implies but does not strictly entail a request, that they be granted permission to go to Cold Stone as well. The parent's first response is an implied denial of this request, on the grounds that the teen hasn't done their chores. The teen responds with an implied promise to finish their chores in the morning, and the parent implies permission with a proviso. 

Implicature is context-dependent; the exchange above assumes that the teen wants to do the same thing as Sasha, that they're responsible for doing the dishes, usually have practice in the mornings, and so on.  In a different context, without these assumptions in play, the implicature doesn't work. 

We can think of a social context as providing unstated assumptions that serve as additional premises, and thereby allow language users to draw additional inferences from a statement that go beyond its strict entailments. Returning to the hypothetical GWAS of educational attainment study, suppose the study found that "educational attainment is 21% heritable." This supports the simplification "educational attainment is substantially heritable." Then, in a context in which educational attainment is assumed to reflect intelligence, this has the implicature "intelligence is substantially heritable." Next, with the genetic/environmental partition assumption and in a context where genetic determinism is widely held, this has the further implicature that "intelligence is innate." Combined with studies that claim differences in mean IQ across racial groups (or even just a contextual assumption to this effect), an assumption that IQ also measures intelligence, and statistical essentialism (the idea that statistical averages reflect group essences), we get to the scientific racist implicature that "some races are innately more intelligent than others." 

In this way, a study that did not mention race — or, indeed, might have mentioned race only to highlight that the sample was racially homogeneous and therefore the results could not be generalized — could still be utilized to support scientific racist implicatures and therefore be race science [@KitcherArgumentFreeInquiry1997; @WillsAreClustersRaces2017; @BirdTypologicalThinkingHuman2024]. (Note that denying the generalizability of results entails that the study is not deductively relevant to the generalized research question.) 

Our example in the main text, of the Flynn effect as legitimizing race science even though it poses an empirical challenge to certain aspects of race science (hereditarianism), can be analyzed in a similar way. A project investigating the Flynn effect might pose a research question that assumes — implicitly or explicitly — that it is scientifically possible but not certain than hereditarianism is false. This entails that it is scientifically possible but not certain that hereditarianism is true. Then, in a context where it is widely assumed that any scientifically possible but uncertain claim is a legitimate subject for scientific research, this supports the implicature that hereditarianism is a legitimate subject for scientific research. In this way, Flynn effect research can be race science discourse. 

Because key assumptions that enable scientific racist implicatures are often implicit and context-specific, it will often be difficult to empirically prove to the satisfaction of everyone concerned that a particular project is (or is not) race science — outside of cases where the research has actually been utilized in actual scientific racist propaganda. Authorities within scientific communities — funders, journal editors, hiring and tenure committees — will need to decide what kinds of evidence are appropriate for adjudicating claims of race science, and whether there is sufficient evidence to reach a conclusion (one way or the other) in any given case. The frameworks we have offered here can help these authorities systematically and empirically — that is to say, scientifically — adjudicate such cases.  But race science is still an unavoidably political concept. 



# Proof of concept {#sec-poc}

{{< include ../scripts/11-proof-of-concept.qmd >}}


# Topic model interpretation and quality assessment {#sec-topic-quality}

After fitting topic models, our first research question was whether we could identify distinctive "race science discourse" topics.  Figures [@fig-gamma-lg; @fig-gamma-md; and @fig-gamma-sm] show the gamma (topic-document) distributions for each value of $k$ for the three vocabularies.  In these figures, each panel corresponds to a single journal-$k$ combination, each row of cells is a single document in that journal, and each column of cells is a single topic in that particular model.  Topics correspond within columns of panels, but not within rows; for example, topic 04 for $k=10$ does not necessarily correspond to topic 04 for $k=20$.  Color intensity indicates the value of gamma for that particular topic-document combination.  

For $k=5, 10$ the distributions are quite noisy, difficult to interpret, and generally don't form very strong visual clusters.  For moderate values of $k$, 20 through 40, coherent bands start to appear for journal-distinctive topics in MQ, BG, Int and, to a lesser degree, PID.  Note that the distinctive topics for MQ and Int do not obviously coincide.  At a low level, these journal-distinctive topics reflect different distributions of noun phrases, and prima facie are likely explained by the relatively narrow scope of these four journals; PR is a generalist journal covering all of psychology and BBS is an interdisciplinary cognitive science journal.  Above about $k=50$, the models appear to identify more fine-grained topics, the coherent bands fade, and the topics do not obviously distinguish different journals.  

We next constructed Silge plots [@SilgeTopicModeling2017], showing the top 15 (highest-probability) terms (noun phrases) from each topic in the model; see figures [@fig-silge-lg; @fig-silge-md; @fig-silge-sm].  Perusing these term lists, we focused on topics in each model that contained racial terms.  Every fitted topic model included at least one racial topic.  Importantly, in some cases these topics contained terms related to intelligence research, but in other cases they did not.  

The gamma distributions and Silge plots suggested that, once $k$ was sufficiently large, the topic models were distinguishing between two types of race science discourse articles.  We therefore examined the prevalence of topics that used racial terms, intelligence-research terms, or both, by journal and across all 24 models; see figures [@fig-grid-lg; @fig-grid-md; @fig-grid-sm]. In these visualizations, each row of facets corresponds to a value of $k$ (excluding 5 as too noisy), and each column of facets corresponds to one topic.  Topics are clustered based on whether the top 15 terms contain racial terms ("race"), terms related to intelligence research ("intelligence"), or both.  Line plots show the count of articles with $\gamma > 0.50$ for the given topic, by journal; thick lines are 5-year running averages and thin lines are raw counts.  

Across all three vocabularies, for sufficiently large values of $k$ the models identify two or more different racial topics, one that appears all but exclusively in MQ and at least one that appears primarily in mainstream journals.  

We next conducted a topic quality check of topic 24 from the medium vocabulary, $k=40$ model, which appeared to identify race science discourse on intelligence published in mainstream journals.  A spreadsheet of all articles that had a maximum value of $\gamma$ for this topic was extracted (excluding articles published in *Psychological Reports*), and the top 121 articles ($\gamma > 0.97$) were reviewed manually by both authors.  We coded each article as *race science discourse*, or not, using our definition of race science discourse as treating race science as a legitimate area of scientific research; this includes methodological critiques of race science and empirical tests that falsify race science hypotheses.  

For the first round of review, both authors worked independently,  dichotomously coding each article as race science discourse or not.  We calculated the "false positive rate" — documents with maximum $\gamma$ in this topic that were not race science discourse — and interrater reliability using Cohen's $\kappa$.  The consensus true positive rate was 89.3% (108/121), while the consensus false positive rate was 8.3% (10/121), with 98% agreement across the two raters ($\kappa = 0.86)$.  Coding spreadsheets are included in the supplemental materials S8-S9.  For each of the 10 consensus false positive documents, their inclusion in the topic was readily explained, but in different ways for different documents.  Some included text from other documents, as when an article started in the middle of a page; others discussed perceptions of race relations or racial animus; one piece in BG discussed differences in the herding behavior of two cattle breeds, referred to as "white" and "black."  

The 3 non-consensus documents reflected essential ambiguity in the operationalization of race science discourse.  One is a table of contents from a 1980 issue of Int; arguably this document should have been excluded as irrelevant front matter, but arguably it legitimizes the research published in the issue.  The second case is a paper on racial differences in emotional intelligence that was classified as race science discourse by one author, while the other author felt it was more focused on psychometrics [@GignacGroupDifferencesEI2010].  And a 1990 article in MQ (<https://www.unz.com/print/MankindQuarterly-1990q3-00108/>) proposed to offer an economic explanation for crime.  While it has a few pages connecting race/ethnicity to both crime and poverty, it ultimately focuses on unemployment and alcohol sales, rather than race, as the key predictors of crime.  

Based on this qualitative review, we judged that the topic model approach was sufficiently sensitive (low "false positive" rate) in identifying race science discourse.  However, "false negatives" are still possible, as some documents that human readers would classify as race science discourse might have been associated with other topics.  

# STM robustness check {#sec-stm}

As a robustness check, we compared the `tmfast` results with a topic model fit using `STM`, a widely used package for topic modeling in R [@RobertsStmPackageStructural2019].  Using the medium vocabulary, $k=40$, and a relaxed convergence threshold (`1e-4` compared to the default `1e-5`), fitting a single `STM` model took approximately 20 minutes.  This was approximately two orders of magnitude slower than `tmfast`.  And, unlike `tmfast`, `STM` can only fit a single value of $k$ at a time.  We therefore only fit this one `STM` model.  

Figure @fig-stm shows four topics extracted from the `STM` model, automatically identified by the use of race keywords, intelligence keywords, or both, among the top 15 terms.  As in the `tmfast` analysis, the "race only" topic is almost exclusive to MQ; the "race and intelligence" topic emerges in mainstream journals before appearing in both the mainstream and MQ; and there are distinctive "intelligence only" topics.  Compare figures @fig-focal, @fig-grid-md.  One notable difference between these results is the role of PR in publishing race and intelligence.  

![Silge plots and thresholded document counts for "race only," "race-and-intelligence," and "intelligence only" topics in the STM model.](img/12-stm.png){#fig-stm fig-env="figure*"}


# Supplemental analysis of *Behavior Genetics* {#sec-bg}

To check the finding that very few race-and-intelligence documents had been published in BG, we ran an independent search for documents with the keywords "race" and "intelligence" published in BG from 1972 to 2020 using Springer's journal search website.  This search returned 125 documents.  Thirty-three were presentation abstracts from meetings of the Behavior Genetics Association; we did not examine these further.  Fourteen of the remaining documents had been published since 2010 (after the scope of the primary study).  Among these, one obituary and two papers celebrated the work of John Loehlin, a prominent race-and-intelligence researcher who had been Director of the American Eugenics Society from 1968-1972 [@TurkheimerJohnLoehlin192620202020; @WaldmanIntroductionFestschriftJohn2014; @PlominGenotypeEnvironmentCorrelationEra2014].  Another was a retrospective of the work of Lindon Eaves, a geneticist who made at least one notable contribution to debates on the Scarr-Rowe hypothesis (interactions between heritability, race, and class).  None of these 14 documents reported any studies of racial differences.  

Twenty-eight documents in this sample from BG were published between 1990 and 2010.  One was an obituary of Jerry Hirsch, a critic of hereditarianism in general and Jensen in particular [@RoubertouxJerryHirsch202008], and another was an obituary of David Rowe [@RodgersObituaryDavidChristian2003].  Only 2 of these 28 documents reported race differences of any kind: one examining interactions among race, sex, and heritability for adolescent BMI (Body Mass Index, used as a measure of overweight/obesity) [@JacobsonGeneticSharedEnvironmental1998]; and the other interactions among race, family history of alcoholism, and visuospatial performance [@BermanReducedVisuospatialPerformance1995].  Finally, Philip Vernon — one of the Pioneer-funded scientists we identified earlier — published a critical review of a book that attempted to address hereditarianism, the Flynn effect, race-and-intelligence research, and some related issues.  

At the same time, we were unable to find any articles published in BG that acknowledged the involvement of the field with scientific racism with anything like the force of the statements that have been made by biological anthropologists [@FuentesAAPAStatementRace2019] and human geneticists [@JacksonFacingOurHistory2023].  Notably, the ASHG report includes discussions of behavior genetics race science, and in particular is critical of the field and the organization for failing to publicly reject the race-and-intelligence claims made by Shockley and Jensen [@JacksonFacingOurHistory2023].  A search for "eugenics" in BG found articles that presented the issue as part of the distant past, and a search for "racism" turned up studies of racist attitudes.  A statement on the Behavior Genetics Association website, dated 2021, acknowledges that "The history of our field is inextricably linked with racism, including the misuse of behavior genetic research to support violent eugenic policies," but primarily focuses on BGA member demographics (<https://www.bga.org/content.aspx?page_id=22&club_id=971921&module_id=567723>).  

# Supplemental analysis of "presence" of Pioneer-funded authors {#sec-presence}

For this supplemental analysis, we focused on the medium vocabulary, $k = 40$ and the same focal topics as in figure @fig-focal. We characterized a paper as "in" one of these topics based on its maximum value of $\gamma$, the topic-document distribution. 

## Author name handling and ambiguity error

The Crossref API does not standardize or disambiguate author names. We manually disambiguated the names of the Pioneer-funded authors, e.g., searching the author field for every occurrence of "Rushton" and then identifying all the variations for J. Philippe Rushton. A simple crosswalk table allowed us to standardize all of these variations to a "canonical" version of the name of each Pioneer-funded author, e.g., J.P. Rushton would be replaced with the "canonical" version J. Philippe Rushton before proceeding with further analysis. Fig. @fig-pf-count shows the number of Pioneer-funded authors who published in the three focal topics, by year. Across the entire scope of the data, 13 of the Pioneer-funded authors ever published in any of these three topics, with Rushton, Lynn, Jensen, Humphreys, and Vernon each publishing 10 or more papers (aggregated across topics).  (Note that Lloyd Humphreys and Robert A. Gordon did not have any results in the Web of Science searches but did have papers in the corpus. Hardin, Horn, and Shuey do not appear in the papers for these focal topics.) 

![Count of Pioneer-funded authors publishing in three focal topics, by year](img/15_pf_count.png){#fig-pf-count}

Names of other authors were not disambiguated, and so, e.g., D.J. Hicks and Daniel J. Hicks would be treated as two different authors for the purposes of this analysis. As a consequence, the total number of authors is probably too high in the estimate of `author_share`, and so the estimate of `author_share` is probably too low. Assuming no issues with the manual disambiguation of Pioneer-funded authors, `paper_share` does not have this problem. Therefore the estimated difference `paper_share - author_share` is probably larger than the true difference. In terms of subfigure @fig-presence E, the vertical bar is probably too far to the right. 

This error is unlikely to have significant implications for topics 7 (MQ-style race science) and 22 (mainstream intelligence research). For topic 24 (race-and-intelligence), we judge that a significant error would be 5 percentage points or more, i.e., rather than the estimated mean difference of about 11%, the true mean difference would be about 6% or less. Algebraically, 

$$ \frac{p}{t} - \frac{p}{r} = \frac{p}{r - \varepsilon} - \frac{p}{r} > \alpha $$

where $p$ is the count of Pioneer-funded authors; $t$ is the true author count, i.e., after perfect disambiguation; $r$ is the "raw" author count estimated using no disambiguation (except for the Pioneer-funded authors); $\varepsilon$ is the "ambiguity error" $r - t$; and $\alpha$ is the threshold .05 for a significant error in the conclusion.  Solving for $\varepsilon$ gives

$$\varepsilon > r - \frac{p}{\alpha + p/r}$$

To estimate what size of ambiguity error $\varepsilon$ would be required, we first note from @fig-pf-count that at most 5 Pioneer-funded authors are active in topic 24 in any given year. And subfigure @fig-presence A indicates about 20-25 authors are active in this topic each year from the mid 1970s on. Setting $p = 5$ and $r = 20$ gives us $\varepsilon > 3.33$; $p = 5$ and $r = 25$ gives $\varepsilon > 5$; and $p = 3, r = 20$ also gives $\varepsilon > 5$. That is, a disambiguation error of 4-5 authors could be enough to undermine our finding for topic 24. 

We therefore manually inspected the list of authors for topic 24 in 1989, 1990, and 1991, to gauge the potential magnitude of ambiguity error. In 1989, topic 24 had 21 distinct authors, including one paper without an author name, and no author name variations that would require disambiguation. In 1990, topic 24 had 24 authors, 1 missing, and no variations. And in 1991 there were 30 authors, none missing, no variations. 

For the purposes of this ad hoc analysis, we are satisfied that ambiguity error is probably sufficiently small. 

## Paper share and presence

Paper share was calculated simply by asking whether any (at least one) of a given paper's authors appeared on our list of Pioneer-funded authors. This resulted in a dichotomous (yes/no) variable at the paper level, and paper share was calculated as the fraction of papers with a "yes" value. 

In subfigure @fig-presence D, years when Pioneer-funded authors had 0 author share, i.e., none of these authors published in the topic, were excluded from calculating the mean value of presence. 


## Resampling to contextualize presence

To contextualize this measure of "presence" within topics, we first identified all distinct authors (Pioneer-funded or otherwise, again without disambiguation for the others) across all papers in the given topic. We then used a permutation-style resampling approach: drawing a set of 13 authors and calculating paper share, author share, their difference, and the mean of this difference, exactly as with the Pioneer-funded authors. (In particular, years where author share was 0, i.e., the authors had no publications in the topic, were excluded from calculating the mean difference.) 

This approach is equivalent to randomly assigning group labels in a permutation test, and can be interpreted as comparing the presence of Pioneer-funded authors to a "random" set of authors.  We conducted 1,000 resamples for each of the three focal topics. 





![Fitted topic models: Gamma (topic-document) distributions, large vocabulary.](img/05_lg_gamma.png){#fig-gamma-lg  fig-env="figure*"}

![Fitted topic models: Gamma (topic-document) distributions, medium vocabulary.](img/05_md_gamma.png){#fig-gamma-md  fig-env="figure*"}

![Fitted topic models: Gamma (topic-document) distributions, small vocabulary.](img/05_sm_gamma.png){#fig-gamma-sm  fig-env="figure*"}

![Silge plots (top 15 terms) for each topic and model, large vocabulary.](img/06-lg-silge.pdf){#fig-silge-lg fig-env="figure*"}

![Silge plots (top 15 terms) for each topic and model, medium vocabulary.](img/06-md-silge.pdf){#fig-silge-md fig-env="figure*"}

![Silge plots (top 15 terms) for each topic and model, small vocabulary.](img/06-sm-silge.pdf){#fig-silge-sm fig-env="figure*"}

![Topics with "race," "intelligence," or both occurring in the top 15 terms, large vocabulary.](img/08_grid_lg.pdf){#fig-grid-lg fig-env="figure*"}

![Topics with "race," "intelligence," or both occurring in the top 15 terms, medium vocabulary.](img/08_grid_md.pdf){#fig-grid-md fig-env="figure*"}

![Topics with "race," "intelligence," or both occurring in the top 15 terms, small vocabulary.](img/08_grid_sm.pdf){#fig-grid-sm fig-env="figure*"}
