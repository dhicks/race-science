% Use only LaTeX2e, calling the article.cls class and 12-point type.
\documentclass[12pt]{article}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% www.sciencemag.org/about/authors/prep/TeX_help/ .
% This package should properly format in-text
% reference calls and reference-list numbers.

% Science template required packages
\usepackage{scicite}

\usepackage{times}

% add-ons for rticles
% based on arxiv.sty
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}

% the basics
\usepackage{amsmath}
\usepackage{calc}
\usepackage{tabularx}

% for figure adjustment
\usepackage{placeins}
\usepackage{flafter}




% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% From pandoc table feature
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}

% Pandoc citation processing



% The following parameters seem to provide a reasonable page setup.

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm
\textheight 21cm
\footskip 1.0cm


%The next command sets up an environment for the abstract to your paper.
\usepackage{setspace}

\newenvironment{sciabstract}{%
\begin{quote} \singlespacing}
{\end{quote}}


% If your reference list includes text notes as well as references,
% include the following line; otherwise, comment it out.

\renewcommand\refname{References and Notes}

% The following lines set up an environment for the last note in the
% reference list, which commonly includes acknowledgments of funding,
% help, etc.  It's intended for users of BibTeX or the {thebibliography}
% environment.  Users who are hand-coding their references at the end
% using a list environment such as {enumerate} can simply add another
% item at the end, and it will be numbered automatically.

\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{\bf Mainstreaming Race Science}


% Place the author information here.  Please hand-code the contact
% information and notecalls; do *not* use \footnote commands.  Let the
% author contact information appear immediately below the author names
% as shown.  We would also prefer that you don't change the type-size
% settings shown here.


\author{
Daniel J. Hicks,\textsuperscript{1}\textsuperscript{*}
and Emilio Lobato\textsuperscript{1}
\\
\\
\normalsize{\textsuperscript{1}University of California, Merced}\\
\\
\textsuperscript{*}Corresponding author: Daniel J. Hicks, \href{mailto:dhicks4@ucmerced.edu}{\nolinkurl{dhicks4@ucmerced.edu}}.
}

% Include the date command, but leave its argument blank.

\date{}



%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%



\begin{document}
% Double-space the manuscript.

\baselineskip24pt

% Make the title.

\maketitle

% Place your abstract within the special {sciabstract} environment.

\begin{sciabstract}
\emph{{[}structured{]}}
\end{sciabstract}

\hypertarget{introduction}{%
\section*{Introduction}\label{introduction}}

\emph{{[}hey you should read this paper{]}}

\emph{{[}incorporate def'ns{]}}

\begin{description}
\tightlist
\item[scientific racism]
purporting to justify racial inequality and colonialism by appealing to the epistemic authority of science
\item[race science]
(pseudo-)scientific research that can be utilized for scientific racism
\item[race science discourse]
treating race science as a legitimate area of scientific research; this includes methodological critiques of race science and empirical tests that falsify race science hypotheses
\end{description}

\emph{{[}Pioneer{]}}

\emph{{[}probably don't include this{]}}
- pre-20th century history of race concepts and race science
- expansion and contraction of racialized terms over time

\hypertarget{brown-v-board-and-mankind-quarterly}{%
\section*{\texorpdfstring{\emph{Brown v Board} and \emph{Mankind Quarterly}}{Brown v Board and Mankind Quarterly}}\label{brown-v-board-and-mankind-quarterly}}

Histories of scientific racism in the second half of the twentieth century have often emphasized the parascholarly journal \emph{Mankind Quarterly} (MQ) \cite{
MehlerFoundationFascismNew1989,
WinstonScienceServiceFar1998,
SchafferScientificRacismAgain2007,
SainiSuperiorReturnRace2019,
WinstonScientificRacismNorth2020,
AdamsMisAppropriationBiological2021,
SainiDraperMillionsPhilanthropic2022
}, with or without its connections to PF. MQ was founded in 1960 by biologist R. Ruggles Gates (1882-1962), psychologist Henry Garrett (1894-1973), and non-academic anthropologist G. Robert Gayre (self-styled as ``Gayre of Gayre and Nigg''; 1907-1996). Gates' professional status was closely tied to eugenics and the explicit scientific racism of the 1920s, and he had been thoroughly marginalized in the wake of World War II \cite{WinstonScienceServiceFar1998}. In contrast, Garrett had been president of the American Psychological Association in 1946 and chair of Psychology at Columbia from 1941 to 1955 \emph{{[}cite{]}}.

In the landmark case \emph{Brown v Board of Education of Topeka} (1954), the US Supreme Court banned \emph{de jure} educational segregation. The Court's decision relied on expert testimony from psychologists and education researchers; but the segregationists had also put forward their own experts, including Henry Garrett \cite{WinstonScienceServiceFar1998, JacksonScienceSegregationRace2005, SchafferScientificRacismAgain2007}. Shortly after \emph{Brown}, Garrett left Columbia and became involved in segregationist and eugenicist efforts, and even the neofascist Northern League.

As part of the segregationist reaction to \emph{Brown}, MQ was created as what contemporary historians and philosophers of science call an ``echo chamber'' \cite{FernandezPintoKnowBetterNot2017}, a favorable venue for race scientists to publish their views, on the grounds that an ``equalitarian dogma'' created a censorious ``taboo'' against their research in mainstream publications \cite{TuckerFundingScientificRacism2002, JacksonMythicalTabooRace2020}. Scholars have shown that echo chambers are a key part of the ``tobacco strategy,'' used by numerous regulated industries --- most infamously tobacco, but also fossil fuels and chemical manufacturing, among others --- to ``manufacture doubt'' and delay regulation \emph{{[}cites{]}}.

In light of the origins of MQ, the significant attention paid to the journal by historians of scientific racism, and contemporary research on the ``tobacco strategy,'' we hypothesized that bibliometric and text mining analyses would confirm that MQ functioned as an echo chamber for Pioneer-funded scientists. This hypothesis was not supported. Instead, mainstream psychology journals served as the echo chambers for race science.

\hypertarget{mankind-quarterly-and-pioneer-funded-researchers}{%
\section*{\texorpdfstring{\emph{Mankind Quarterly} and Pioneer-funded researchers}{Mankind Quarterly and Pioneer-funded researchers}}\label{mankind-quarterly-and-pioneer-funded-researchers}}

We identified 16 researchers who had received funding from Pioneer; 14 of these researchers had profiles in the Web of Science (WoS) author search, allowing us identify 13 WoS-indexed journals that had published 6 or more of these authors. See Table \ref{tab:researchers}.

\begin{longtable}[]{@{}lrr@{}}
\caption{\label{tab:researchers} Pioneer-funded researchers. Either identified in \emph{{[}cite{]}} or named on Pioneer's website, along with birth and death dates from Wikipedia, identified discipline, and WoS author search result counts. Brunetto Chiarelli does not have a Wikipedia page.}\tabularnewline
\toprule
\endhead
Thomas J. Bouchard, Jr.~(1937-) & psychology & 184 \\
Brunetto Chiarelli (?-?) & anthropology & 91 \\
Hans Eysenck (1916-1997) & psychology & 661 \\
Robert A. Gordon (1932-) & sociology & 0 \\
Linda Gottfredson (1947-) & psychology & 71 \\
Garrett Hardin (1915-2003) & ecology & 75 \\
Joseph M. Horn (1940-) & psychology & 68 \\
Lloyd Humphreys (1913-2003) & psychology & 0 \\
Arthur Jensen (1923-2003) & psychology & 235 \\
Michael Levin (1943-) & philosophy & 96 \\
Richard Lynn (1930-) & psychology & 288 \\
R. Travis Osborne (1913-2013) & psychology & 59 \\
J. Phillippe Rushton (1943-2012) & psychology & 277 \\
Audrey M. Shuey (1900-1977) & psychology & 10 \\
Philip A. Vernon (1950-) & psychology & 227 \\
Daniel Vining, Jr.~(1944-) & demography & 33 \\
\bottomrule
\end{longtable}

\begin{figure}
\centering
\includegraphics[width=4.76in,height=2.6in]{img/wos_results.png}
\caption{Journals publishing 6 or more Pioneer-funded researchers, Web of Science author search results. \label{fig:wos}}
\end{figure}

Figure \ref{fig:wos} shows that, while MQ is among the ``Pioneer-publishing'' journals, a number of mainstream journals are more prominent: \emph{Personality and Individual Differences} (PID), \emph{Intelligence} (I), \emph{Behavior Genetics} (BG), and \emph{Psychological Reports} (PR). In addition, only psychologist Richard Lynn appears to have published heavily in MQ. Lynn has been an assistant editor of MQ since 1979 \emph{{[}check this: \url{https://rationalwiki.org/wiki/Mankind_Quarterly}{]}} and president of PF since the death of psychologist J. Phillippe Rushton in 2012 \emph{{[}cite{]}}. By contrast, a number of Pioneer-funded researchers have published in PID, I, and to a lesser degree BG: Bouchard, Eysenck, Jensen, Rushton, Vernon, and also Lynn.

\emph{Personality and Individual Differences} (PID) was founded in 1980, with Eysenck as editor-in-chief and an editorial board including Jensen and Lynn. In the inaugural editorial, Eysenck identified ``studies of the genetic determinants of individual differences in the areas of personality and intelligence'' as one of the journal's eight major areas of interest. Eysenck remained editor-in-chief until his death in 1997. In 2005 the editorial board still included Jensen and Lynn. PID was first published by Pergamon Press, a mainstream academic press, and today is published by Elsevier. According to the Scimago Journal Rankings, based on Scopus citation data, in 2022 PID had an H-index of 193, \#29 in Psychology (Miscellaneous), and a 2-year citation index (impact factor) of 5.27, \#24 in Psychology (Miscellaneous).

\emph{Intelligence} (I) was founded in 1977, with psychologist Douglas Detterman as editor-in-chief from the founding until 2016. Lloyd Humphreys was on the editorial board starting from 1977; by 1990 he had been joined by Jensen and Philip Vernon. Richard Lynn joined the editorial board sometime between 1998 and 2002. (Archive copies of the I editorial board page are not available from the journal's website from 1999 through 2001.) I has been criticized for including Lynn and Gerhard Meisenberg on its editorial board until July-August 2018 \cite{SainiSuperiorReturnRace2019}. Meisenberg is also connected with the Pioneer Fund \emph{{[}director?{]}}, and was editor-in-chief of MQ in 2015-18 \emph{{[}check these{]}}. I is published by Elsevier. According to the Scimago Journal Rankings, based on Scopus citation data, in 2022 I had an H-index of 103, \#32 in Experimental and Cognitive Psychology, and a 2-year citation index (impact factor) of 3.05, \#31 in Experimental and Cognitive Psychology.

\hypertarget{topic-model-analysis-identifies-race-science-discourse}{%
\section*{Topic model analysis identifies race science discourse}\label{topic-model-analysis-identifies-race-science-discourse}}

The fact that Pioneer-funded researchers published heavily in two mainstream psychology journals does not tell us anything about the content of their publications or the findings of their research. We assembled a full-text corpus of articles published in MQ, PID, I, BG, PR, and Behavior and Brain Sciences (BBS) between 1960-2010 and used topic modeling to identify topics discussing race, intelligence, and both. \emph{{[}supplemental tables: Ns for each article, topic model parameters; supplemental figs: articles by pub over time, topic heatmaps, Silge plots, race-intelligence-both grids{]}}

\begin{figure}
\centering
\includegraphics[width=4.76in,height=3.57in]{img/focal_topics.png}
\caption{Silge plots \cite{SilgeTopicModeling2017} and smoothed time series for three focal topics. Top row: top 15 terms by \(\beta\) (term-topic distribution) for each topic. Bottom row: count of articles associated with each topic, by journal and year. Article counts use a threshold approach, with \(\gamma\) (topic-document distribution) greater than 0.5. Thin lines give annual values, thick lines give 5-year running averages. Because \emph{Behavior Genetics} and \emph{Behavioral and Brain Sciences} are not prominent in any panel, neither is given a direct label. \label{fig:focal}}
\end{figure}

We focus here on three topics identified in one of the 24 fitted topic models; see Figure @(fig:focal). In this model, topic 07 is strongly associated with MQ: MQ published dozens of articles in this topic each year, and no other journal ever published more than a handful, with most other journals publishing none in a given year. The Silge plot \cite{SilgeTopicModeling2017} contains racial terms (\texttt{races}, \texttt{whites}, \texttt{negroes} and potentially \texttt{europe}, \texttt{africa}, \texttt{india}, and \texttt{japan}) as well as \texttt{book}, likely reflecting the fact that MQ published a number of book reviews while the psychology journals did not.

Topic 22 is strongly associated with I and PID in the same way. The Silge plot does contain \texttt{jensen}, as well as a reference to Raymond Cattell, who played a major role in the development of factor analysis and intelligence testing also advocated eugenics, fascism, and Nazi race science\cite{MehlerBeyondismRaymondCattell1997}. However, the other authors named in this topic --- John Horn and Peter Bentler --- do not appear to have contributed to race science discourse. Instead this topic appears to identify ``mainstream'' (non-race science) intelligence research, specifically on factor analysis and the debate over whether intelligence in unidimensional or multidimensional. (Three other topics only associated with mainstream intelligence journals and terms were also identified by this topic model.) This topic indicates that the model is not simply lumping race science research together with other intelligence research.

The Silge plot for topic 24, by contrast, suggests a distinct race science discourse topic, with multiple racial terms (\texttt{whites}, \texttt{blacks}, \texttt{racial\_differences}, \texttt{races}, \texttt{race\_differences}) and the names of three prominent Pioneer-funded researchers, \texttt{jensen}, \texttt{lynn}, and \texttt{rushton}. To check this interpretation, we independently coded the top 121 papers in this topic (those with \(gamma > 0.97\)) as contributing to race science discourse or not. 105 (87\%) were coded as race science discourse by both authors; 10 (8.3\%) were coded as not race science discourse by both; and discordant codes were given to the remaining 6 (5.0\%) (95\% agreement, Cohen's \(\kappa = 0.74\)).

In almost all years, most papers in topic 24 were published in mainstream journals rather than MQ. Jensen's ``How Much Can We Boost IQ and Scholastic Achievement?'' \cite{JensenHowMuchCan1969} was published in 1969 in Harvard Educational Review (not included in this study), and the time series indicates that, around this time, there was an increase in articles in topic 24 in both PS and MQ (the only two journals in our corpus that were active at the time). MQ shows another sharp increase in the late 1970s, around the time Lynn was brought on as assistant editor, \emph{{[}confirming the claim that Lynn shifted MQ's focus to intelligence research{]}}. PID published multiple papers in topic 24 almost immediately after it was founded, with I showing a more gradual increase between the mid-1970s and mid-1990s.

\hypertarget{race-science-and-behavior-genetics}{%
\section*{Race science and behavior genetics}\label{race-science-and-behavior-genetics}}

Notably, BG published very few articles in this topic. Outside of the field itself, behavior genetics is strongly associated with race science by both other academics and the general public. Panofsky \cite{PanofskyMisbehavingScienceControversy2014} argues that, prior to Jensen's 1969 paper, behavior genetics emphasized the study of non-human animals and intentionally avoided associations with eugenics and public controversy more generally. In response to Jensen, critics such as Lewontin offered broad critiques of behavior genetics as such, and behavior geneticists in turn adopted a radical conception of academic freedom (without any sense of responsibility for the social implications of academic research) and a siege or wartime mentality, as demonstrated by Sandra Scarr's 1986 presidential address to the Behavior Genetics Association (BGA) \cite{ScarrThreeCheersBehavior1987}. Scarr's address coincided with the period between 1970-1990 when BG published articles in topic 24 (including Scarr's address itself).

However, Glayde Whitney's 1995 BGA presidential address was published in MQ rather than BG. According to Whitney, BG's editor refused to publish the address and the BGA Executive Committee (except for Whitney himself) voted to ``issue an official statement of denouncement'' of the address; though we were unable to find any such a statement, and Panofsky reports that ``nothing official was done to Whitney.'' In his address, Whitney criticized ``the Marxist-Lysenkoist denial of genetics''; quoted Peter Brimelow --- a white nationalist and later founder of the racist and xenophobic website VDARE --- defining ``racist'' as ``anyone who is winning an argument with a liberal''; and proposed as a ``reasonable scientific hypothesis'' that differences in murder rates between countries and cities was caused by racial genetic differences in intelligence, empathy, aggression, and impulsivity.

The topic model analysis and the case of Whitney's address suggest that, by the 1990s, behavior genetics may have somewhat distanced itself from race science, though perhaps without directly repudiating it \cite{GillbornSoftlySoftlyGenetics2016, HennWhyDNANo2021}.

\emph{{[}move most of this to supplement?{]}}
To check this interpretation, we ran an independent search for documents with the keywords ``race'' and ``intelligence'' published in BG from 1972 to 2020, using Springer's journal search website. This search returned 125 documents. 33 were presentation abstracts from meetings of the Behavior Genetics Association; we did not examine these further. 14 of the remaining documents were published since 2010 (after the scope of the primary study). Among these, one obituary and two papers celebrated the work of John Loehlin, a prominent race-and-intelligence researcher who had been Director of the American Eugenics Society from 1968-1972 \cite{JohnLoehlin19262020, WaldmanIntroductionFestschriftJohn2014, PlominGenotypeEnvironmentCorrelationEra2014}. Another was a retrospective of the work of Lindon Eaves, a geneticist who made at least one notable contribution to debates on the Scarr-Rowe hypothesis (interactions between heritability, race, and class). None of these 14 documents reported any studies of racial differences.

28 documents in this sample were published between 1990 and 2010. One was an obituary of Jerry Hirsch, a critic of hereditarianism in general and Jensen in particular \cite{RoubertouxJerryHirsch202008}, and another was an obituary of David Rowe \cite{RodgersObituaryDavidChristian2003}. Only 2 of these 28 documents reported race differences of any kind: one examining at interactions among race, sex, and heritability for adolescent BMI (Body Mass Index, used as a measure of overweight/obesity) \cite{JacobsonGeneticSharedEnvironmental1998}; and the other at interactions among race, family history of alcoholism, and visuospatial performance \cite{BermanReducedVisuospatialPerformance1995}. Finally, Philip Vernon --- one of the Pioneer-funded scientists we identified earlier --- published a critical review of a book that attempted to address hereditarianism, the Flynn effect, race-and-intelligence research, and some related issues.

This supplemental review provides additional support for the interpretation that behavior genetics had distanced itself from race science by the 1990s. Notably, this review brought our attention to a disciplinary history of behavior genetics published in 2010 \cite{McGueEndBehavioralGenetics2010}, which acknowledges the historical association with eugenics but does not mention Jensen and other race science controversies since World War II. Instead postwar behavior genetics is portrayed as an endeavor to counterbalance the hegemony of the ``Blank Slate,'' the supposedly widespread view that genetic differences play no role in explaining individual differences \cite{BatesonCorpseWearisomeDebate2002}. And none of the three documents celebrating Loehlin noted his postwar involvement in the eugenics movement. None of the documents examined in this supplemental review explicitly repudiate race science. Similarly, another disciplinary history of behavior genetics, published by Plomin in 2023 \cite{PlominCelebratingCenturyResearch2023}, briefly mentions the controversies surrounding Jensen's 1969 paper and later \emph{The Bell Curve}, writing that Jensen's work was controversial because he ``touched on ethnic differences''; though ultimately Plomin reassures the reader that ``by the 1990s, quantitative genetic research had convinced most scientists of the importance of genetics for behavioral traits, including intelligence.'' In short, behavior geneticists seem to typically adopt a posture of objectivity and technical rigor, downplaying the extent of the field's involvement in race science in the past, and displacing the source of race science controversies to external, ``political,'' and ``irrational'' actors.

\hypertarget{discussion}{%
\section*{Discussion}\label{discussion}}

Using bibliometric and text-mining methods, this study finds that Pioneer-funded researchers published heavily in certain mainstream psychology journals, rather than \emph{Mankind Quarterly}; and that a distinct topic of race science discourse can be identified in these same journals across the period 1960-2010. These findings indicate that \emph{Mankind Quarterly} (MQ) was less important as an echo chamber for late 20th century race science than mainstream psychology journals, especially \emph{Intelligence} (I) and \emph{Personality and Individual Differences} (PID). Indeed, MQ published a very different kind of race science from the race-and-intelligence research published in I and PID.

We stress that we are not identifying any particular scientist as a racist, scientific racist, or race scientist based on the topic model results. Scientific research can be utilized to promote scientific racism --- and thus count as race science --- independently of the views and attitudes of the scientist who conducted that research.\cite{TaberyWhyStudyingGenetics2015, GillbornSoftlySoftlyGenetics2016, CarlsonQuantifyingContextualizingImpact2020, HennWhyDNANo2021} But this does not mean that scientists are not responsible for the downstream social effects of their
research, even when these effects involve deliberate misinterpretation.\emph{{[}Douglas, Kitcher, Kourany, Havstad; Block and Dworkin?{]}} Like other citizens, scientists are responsible for mitigating the reasonably foreseen harms that result from their actions. And we now have decades of empirical evidence that research on the genetics of intelligence --- even research that does not itself touch on race --- can and will be used to promote scientific racism \cite{MeyerWrestlingSocialBehavioral2023}. Specifically, while our findings suggest that behavior genetics has attempted to distance itself from scientific racism, behavior genetics research --- and research on human genetics more generally --- may be especially susceptible to appropriation by scientific racists \cite{CarlsonQuantifyingContextualizingImpact2020, OgbunugaforDNABasketballBirthday2022, *[Harden reviews]*}.

Our distinction between race science and race science discourse highlights an important ``engagement dilemma'' for scientists challenging race science and scientific racism. While the ``tobacco strategy'' for manufacturing doubt involves targeting mainstream scientists and promoting fringe scientific views, its primary audiences are lawmakers and the general public. The aim of the strategy is not to change the opinion of mainstream scientists, but instead to cultivate the impression that there is legitimate scientific uncertainty about the issue, reasonable disagreement among qualified experts \emph{{[}cites{]}}. On the one hand, refusing to engage with fringe scientists --- or, often, pseudoscientists \cite{BhakthavatsalamVirtueEpistemologicalApproach2021} --- means that false or misleading statements go unchallenged. On the other hand, detailed, technical criticism legitimizes the fringe views, creating exactly the impression of reasonable scientific disagreement that the tobacco strategy is intended to produce.

In the case of race science, mainstream psychologists have often taken a non-engagement approach --- whether due to a strong conception of academic freedom or a desire to avoid stoking controversy --- with the result that race scientists have been able to present themselves as the representatives of ``mainstream science on intelligence.'' On the other hand, critics --- most prominently Gould, Kamin, and Lewontin --- have offered sophisticated technical critiques. But these tend to spiral off into esoteric debates that are inaccessible to the general public, as with ``Lewontin's fallacy'' \cite{RosemanLewontinDidNot2021}. Consequently these critiques are likely to be either ignored by the general public or taken as evidence of legitimate scientific debate.\\
\emph{{[}more?{]}}

\hypertarget{limitations}{%
\subsection*{Limitations}\label{limitations}}

A key limitation of the current study stems from the inability of text mining techniques to draw on a broader cultural context than what is represented in the corpus. Scholars of racism and US race relations have noted a shift in the way racist attitudes are understood and expressed, from \emph{{[}something{]}} to ``colorblind racism'' \emph{{[}Bonilla-Silva{]}}, defined as \emph{{[}defn{]}}. Colorblind racism involves the use of racialized ``codes'' or ``dogwhistles,'' language that is superficially non-racial but carries racial implications, such as (in the US context) ``violent inner city criminals'' or ``welfare queen'' \cite{SaulDogwhistlesPoliticalManipulation2018}. Alderfer notes that the racist implications of \emph{The Bell Curve} only emerge gradually, with the opening of the book using the colorblind language of ``group differences'' \cite{AlderferScienceNonsciencePsychologists2003}. Panofsky et al.~show how the term ``human biodiversity,'' originally developed for anti-racist purposes by biological anthropologist Jonathan Marks, has been inverted by white supremacists and is now a dogwhistle for biological racial hierarchy \cite{PanofskyHowWhiteNationalists2021}. These cases suggest further that colorblindness can facilitate white supremacist appropriation of non-racial research --- even contrary to the researchers' own intentions --- as scientific racist readers re-interpret racial-neutral language as dogwhistles \cite{GillbornSoftlySoftlyGenetics2016, WillsAreClustersRaces2017}.

Techniques such as topic modeling may be able to account for discursive shifts from explicit to coded racial language, but only if the corpus contains a critical mass of documents using both sets of terms together. This would seem to require either a relatively gradual transition, the inclusion of reflective commentary, such as scholarship identifying and analyzing the transition, or both. It is unlikely that these requirements are met by the corpus used in this study. So it is plausible that the topic models used here have some rate of ``false negatives,'' documents in the corpus that engage in race science discourse but in a colorblind way that avoids the use of the explicit racial terms in the race-and-intelligence topic.

The phenomenon of colorblind racism is closed related to ambiguities that cannot be resolved even by human coders familiar with the broader cultural context. Two relevant examples are the ``Flynn effect,'' a secular increase in intelligence test scores noted by philosopher James Flynn; and studies of ``national IQ,'' exemplified by the work of Richard Lynn. The Flynn effect implies that environmental factors can create group differences in IQ that are comparable to Black-White differences, and thus directly challenges claims that the racial differences are biological. Using methods widely regarded as ad hoc, cherry-picking, and generally unreliable, Lynn and collaborators have claimed that national IQ averages are correlated with national GDP. Both the Flynn effect and ``national IQ'' are strongly associated with race-and-intelligence discourse in the US cultural context. But scientific journal articles on these topics may not include any racial language at all. For example, a meta-analysis of the Flynn effect does not discuss racial differences or use terms such as ``White'' or ``Black'' outside the reference list \cite{TrahanFlynnEffectMetaanalysis2014}. Should such articles be coded as race science discourse? We expect that even readers who accept our definitions of race science and race science discourse will reasonably disagree on how to answer this question. Topic models, of course, are not even capable of representing this essential ambiguity.

\emph{{[}what can science do? Posselt, /Equity in Science/{]}}

\emph{{[}max \textasciitilde50 refs{]}}

\appendix

\hypertarget{materials-and-methods}{%
\section*{Materials and methods}\label{materials-and-methods}}

\hypertarget{corpus-assembly}{%
\subsection*{Corpus assembly}\label{corpus-assembly}}

\begin{figure}
\centering
\includegraphics[width=8in,height=6.1in]{img/dataset.png}
\caption{Corpus assembly. \label{fig:corpus}}
\end{figure}

Corpus assembly is summarized in figure \ref{fig:corpus}. The corpus was assembled in two parts, one for \emph{Mankind Quarterly} and the other for mainstream journals.

\hypertarget{mankind-quarterly}{%
\subsubsection*{Mankind Quarterly}\label{mankind-quarterly}}

An electronic archive of \emph{Mankind Quarterly}, from the first issue in 1969 through 2004, is available for free on the open web at the white nationalist website \emph{The Unz Review}. The Python package Beautiful Soup (\url{https://www.crummy.com/software/BeautifulSoup/}, version not recorded) was used to retrieve every PDF available in this archive. Text was extracted using the R package \texttt{pdftools} \cite[3.0.1]{OomsPdftoolsTextExtraction2023}. This stage of corpus assembly was conducted in Fall 2021 with the assistance of Anthony Sainez.

\hypertarget{mainstream-journals}{%
\subsubsection*{Mainstream journals}\label{mainstream-journals}}

To identify suitable mainstream journals for inclusion in the corpus, we first identified academic researchers who had been funded by the Pioneer Fund. Pioneer is an American non-profit organization founded in 1937 to fund eugenics research and propaganda \cite{TuckerFundingScientificRacism2002}. After the \emph{Brown v. Board of Education} ruling in 1954, Pioneer funded various segregationist efforts across the United States, including lectures on eugenics by Stanford physicist William Shockley \cite{JacksonScienceSegregationRace2005}. We reviewed a critical profile of Pioneer \cite{MillerPioneerFundBankrolling1994} as well as an archived page from the organization's own web site \url{https://web.archive.org/web/20130103005545/http://www.pioneerfund.org/Grantees.html}, which together listed 16 researchers who had received Pioneer funds.

We then used the author search tool in Clarivate's Web of Science platform \emph{{[}url{]}}, retrieving publication lists for 14 researchers. These searches were conducted between 2021-09-24 and 2021-10-05 by DJH. After parsing these results, we counted how many of the 14 researchers had published in each journal. 13 journals had published 6 or more of the 14 Pioneer-funded researchers. We excluded \emph{Mankind Quarterly} (as already included) as well as \emph{Science} and \emph{Nature} (as too general) from further consideration in this side of the corpus. 3 journals published by the American Psychological Association (APA; \emph{American Psychologist}, \emph{Contemporary Psychology}, and \emph{Journal of Educational Psychology}) had to be excluded due to confusion over who could give us permission to use the archives for a text mining project, with both APA and ProQuest asserting that we needed to get permission from the other entity. \emph{European Journal of Personality}, published by SAGE, also had to be excluded because our institutional access only went back to 1999.

The remaining 5 journals are all published by major academic publishers --- Elsevier, Springer, or Cambridge University Press --- and each item in the entire run of each journal has been assigned a DOI (digital object identifier) for archival purposes. We used the Crossref API and \texttt{rcrossref} R interface to this API \emph{{[}cite{]}} to retrieve metadata for each item published from 1960-2010 in each of these 6 journals. These metadata included item-level license information --- confirming that the text of each item could be used for text mining projects --- and a URL to an electronic version of the item. These URLs were used to retrieve an HTML or PDF version of each item, except for \emph{Personality and Individual Differences}. This journal has published a relatively large number of non-article documents, such as book reviews and commentaries, which are not available at the URL included in the Crossref metadata. (This is unfortunate, as it was not difficult to find highly relevant documents that we could not automatically retrieve and therefore excluded from our corpus \emph{{[}review{]}}.) Instead we used Elsevier's ScienceDirect API \emph{{[}url{]}} to search and retrieve all items from \emph{Personality and Individual Differences} that are available. PDFs were run through OCR and text extraction as necessary, as with the \emph{Mankind Quarterly} documents.

\emph{Behavioral and Brain Sciences} typically uses a target article + commentary format; in some cases there are dozens of commentaries for a single article. In the Crossref DOI metadata, each target article and individual commentary is given its own DOI, with no distinction between contribution types or metadata links between a commentary and the corresponding target article. But text is only available in aggregate PDFs that bundle together the target article and all of the commentaries. In a few cases this results in PDFs that are hundreds of pages long and might be linked from the Crossref metadata 30+ times. In addition, the retrieved PDFs are not perfectly identical, because Cambridge UP's servers add a timestamped watermark to each page. We attempted to contact Cambridge for assistance but did not receive a response. We ultimately used a series of ad hoc measures to mitigate text duplication. Approximately 100,000 documents of 1-2 pages long were excluded before PDF retrieval. After PDF retrieval and text extraction, the timestamped watermarks were removed using a regular expression and the text was hashed using SHA 256. Hashes were used to construct groups of duplicate documents, and a single document (whichever one happened to be first in the dataframe) was chosen from each hash group for inclusion in the corpus. It is plausible that some duplicates made it through this process: where the watermark overlapped with the text, the regular expression may have been unable to identify and remove the watermark (a false negative result), and this difference in the watermarks (not the text) would produce different hashes.

\begin{figure}
\centering
\includegraphics[width=4.76in,height=2.6in]{img/02_count.png}
\caption{Count of documents in the corpus, by journal and year. \label{fig:counts}}
\end{figure}

\begin{longtable}[]{@{}lrrr@{}}
\caption{\label{tab:counts} Document counts, by journal, and years included in the corpus.}\tabularnewline
\toprule
journal & n & start & end \\
\midrule
\endfirsthead
\toprule
journal & n & start & end \\
\midrule
\endhead
Behavior Genetics & 2268 & 1970 & 2010 \\
Intelligence & 1237 & 1977 & 2010 \\
Mankind Quarterly & 1821 & 1960 & 2004 \\
Personality and Individual Differences & 7274 & 1980 & 2010 \\
Psychological Reports & 21398 & 1960 & 2010 \\
Behavioral and Brain Sciences & 898 & 1978 & 2010 \\
\bottomrule
\end{longtable}

All together, the corpus includes 34,896 documents from 6 journals. See \ref{fig:counts} and \ref{tab:counts}.

\hypertarget{text-preparation}{%
\subsection*{Text preparation}\label{text-preparation}}

After document retrieval and text extraction, we pre-processed the text using the \texttt{spaCy} NLP (natural language processing) Python library and the R API \texttt{spacyr} \emph{{[}version, cites{]}}. Specifically, we applied regular expressions to remove header/footer copyright notices and hyphenation, used spaCy to annotate and extract noun phrases (eg, ``the intelligence test items''), and then cleaned and standardized these phrases (eg, removing the/an/a, coverting all text to lowercase, and replacing all whitespace with underscores: ``intelligence\_test\_items''). We then counted the occurrence of each noun phrase in the document. The aggregated ``document-term matrix'' was stored in Parquet format \emph{{[}cite{]}} for performance reasons, and written and read using the \texttt{arrow} package for R \emph{{[}cite{]}}. \emph{{[}availability{]}}

NLP-extracted noun phrases offer a number of advantages over the more traditional unigram (``single-word'') tokens. First, noun phrase extraction removes many standard stopwords (articles, common verbs) without relying on a fixed, a priori stopword list. Phrases can be more informative than single terms, for example, distinguishing ``intelligence test'' from ``hypothesis test.'' But simple n-gram extraction will include numerous phrases that are not especially meaningful. Consider the sentence ``Since our first analyses of feeding patterns in rats, we had been using a criterion of 40 minutes (Le Magnen \& Tallon 1963; 1966)'' *{[}10.1017\_\_\^{}\_\_s0140525x0000042x{]}*. Bigrams such as ``since our'' and ``criterion 40'' will likely be discarded in vocabulary selection, but significantly increase the computational cost of vocabulary selection. Noun phrase extraction is therefore more efficient.

After noun phrase extraction, the corpus comprised \emph{{[}tokens{]}} of \emph{{[}distinct noun phrases{]}}.

\hypertarget{data-and-code-availability}{%
\subsubsection*{Data and code availability}\label{data-and-code-availability}}

Document metadata retrieved from Crossref does not appear to be covered by any copyright or other intellectual property restrictions. However, due to copyright restrictions, we are unable to make document fulltext or Web of Science search results publicly available. Code for the corpus assembly and text preparation steps described above is available by request.

The public analysis repository, \url{https://github.com/dhicks/race-science} \emph{{[}DOI{]}}, includes document metadata and documentwise counts of NLP-extracted noun phrases (``document-term matrices''), along with code to reproduce the analysis of the following sections. \emph{{[}repo readme: requires R 4.1.2; rig (\url{https://github.com/r-lib/rig/}) for managing multiple R installations{]}}

\hypertarget{vocabulary-selection}{%
\subsection*{Vocabulary selection}\label{vocabulary-selection}}

We took an information-theoretic approach to vocabulary selection. Consider a game in which I draw a document from the corpus, then a single token from that document. I tell you the term, and you have to guess which document I picked. Intuitively, highly informative terms (in this project, noun phrase types) are distinctive, allowing you to dramatically narrow down the list of potential documents. This ``informativeness'' of a term can be quantified as the KL (Kullback-Leibler) divergence from a ``baseline'' distribution of documents to the distribution conditional on the term. Because the most informative terms tend to be typos and OCR errors --- these are unique to a single document --- we multiply the KL divergence by the logarithm of the total number of occurrences of the term across the entire corpus. We refer to the resulting measure as \(log(n) \Delta H\) or simply \texttt{ndH}.

\emph{{[}Hicks 2021?{]}} used the \texttt{ndH} approach in a topic modeling analysis, where the baseline distribution was the uniform distribution across documents. In the current project, we found that this approach heavily favored recurrent noun phrases in the longest documents. Many of the documents published in \emph{Psychological Reports} are extremely short, 1-2 reports of a single study; while many of the documents published in \emph{Brain and Behavioral Sciences} are book-length collections that include a long review article and sometimes dozens of commentaries. Very generic noun phrases that happen to appear in the latter can occur orders of magnitude more often than highly distinctive phrases in the former, and so the \(log(n)\) factor overwhelms the \(\Delta H\) factor.

To address this, we used a different baseline distribution of documents, namely, one in which document probability is proportional to length. This makes phrases from short documents much more ``surprising'' (much less likely to occur according to the baseline), and hence substantially increases their informativeness. This was more effective at identifying useful phrases from across the corpus.

A common rule of thumb in topic modeling is that the vocabulary should have about 10 times as many distinct terms as the number of documents in the corpus \emph{{[}cite{]}}. However, we had some concerns with computational demands here: the resulting document-term matrix would have roughly \(10 \times 33,000^2\) or 10.9 billion entries; with 10\% density this would require on the order of 4 GB of memory just for a single copy of the matrix; and most of the analysis was to be conducted on the authors' laptops. We therefore chose to work with three smaller vocabularies, \(5 \times\), \(1 \times\) and \(\frac{1/5} \times\) the number of documents. We refer to these as the ``large,'' ``medium,'' and ``small'' vocabulary, respectively, and they include \emph{{[}counts{]}} distinct phrases. We compare findings across vocabularies as a robustness check.

\hypertarget{topic-modeling}{%
\subsection*{Topic modeling}\label{topic-modeling}}

To fit topic models, we followed the approach proposed by \emph{{[}cite{]}}, which uses varimax-rotated partial principal components instead of the variational inference methods used by standard topic model packages \emph{{[}eg, stm{]}}. This novel approach was implemented in the R package \texttt{tmfast}, \emph{{[}cite{]}}. A simulation study of \texttt{tmfast} found that it was significantly faster and only slightly less accurate at reconstructing known word-topic and topic-document distributions, compared to the standard topic modeling package \texttt{stm}.

Topic models were fit for all three vocabularies (large, medium, and small) with \(k = 5, 10, 20, 30, 40, 50\) (number of topics), resulting in a total of 18 models.

Following the approach of \emph{{[}Hicks 2021?{]}}, we did not attempt to identify a unique best model for further analysis. While we focus on the medium vocabulary, \(k=30\) model as the ``median'' among the 18 models fit, we compare and contrast findings from this model with those from the other models.

\hypertarget{topic-model-interpretation}{%
\subsubsection*{Topic model interpretation}\label{topic-model-interpretation}}

After fitting topic models, our first research question was whether we could identify distinctive ``race science'' topics. We addressed this question in two ways. Following common practice in topic modeling, we extracted lists of the top \(n\) (highest-probability) terms (noun phrases) from each topic in each model, with \(n\) generally ranging between 5 and 15 as we explored the models. \emph{{[}tables + Silge plots{]}}

\emph{{[}drop{]}}
We also adapted the ``discursive space'' analysis proposed by \emph{{[}Hicks 2021?{]}}. This analysis combines the Hellinger distance between topic-document distributions --- capturing how ``far apart'' two documents are --- with dimensional reduction techniques used for data visualization. This produces a two-dimensional visualization of individual documents, arranged so that documents with similar topic distributions are visually clustered and visually distinct from documents with different topic distributions. \emph{{[}availability{]}}

Specifically, for each fitted topic model, we extracted topic-document distributions for each document and calculated pairwise Hellinger distances. We then processed these distances using the UMAP algorithm for dimension reduction, as implemented in the \texttt{umap} package \emph{{[}details; cite{]}}. To reduce computational requirements for these steps, we first dropped documents published in the journal \emph{{[}Psychological Reports{]}} (\emph{{[}x documents removed, y remaining{]}}; note that the full corpus was used to fit the topic models, \emph{Psychological Reports} was removed just for this visualization). Using the \texttt{plotly} library \emph{{[}cite{]}}, we construct an interactive visualization: each point represents a single document, and points can be colored based on either their journal or highest-probability topic. Tooltips are displayed when we hover over a point, including document metadata (title, journal, authors, year) as well as ``key phrases.''

To calculate key phrases for a given document \(d\), we first identified the highest-probability topic \(t\) for that document and the word-topic (noun phrase-topic) distribution \(\beta_{w,t}\) for that topic. We also retrieved the document-term matrix, containing the count \(n_{w,d}\) of occurrences for each word (noun phrase), and calculated a weighting \(n_{w,d} \beta_{w,t}\) for each word. The key phrases are the 5 most highly-weighted terms for the given document.

\hypertarget{topic-quality-assessment}{%
\subsubsection*{Topic quality assessment}\label{topic-quality-assessment}}

\emph{{[}rewrite{]}}

We next conducted a topic quality check of topic 19 from the medium vocabulary, \(k=30\) model. Keyword and inspection of the UMAP interactives seemed to indicate that this topic was a ``race science discourse'' topic in this model.

To confirm this initial impression, a spreadsheet of all articles with \(\gamma > 0.25\) for this topic was extracted (excluding articles published in \emph{Psychological Reports}), and the top 200 articles were reviewed manually by both authors. For this qualitative analysis, we defined \emph{scientific racism} as (purporting) to justify racial inequality and colonialism by appealing to the epistemic authority of science; \emph{race science} as (pseudo-)scientific research that can be utilized for scientific racism; and \emph{race science discourse} as treating race science as a legitimate area of scientific research; this includes methodological critiques of race science and empirical tests that falsify race science hypotheses. Our qualitative review of the top-200 topic-19 articles focused on classifying them as \emph{race science discourse}.

For the first round of review, both authors worked independently, dichotomously coding each article as race science discourse or not. We calculated the ``false positive rate'' --- documents with a very high \(\gamma\) in this topic that were not race science discourse --- and interrater reliability using Cohen's \(\kappa\).\\
\emph{{[}false positive rate{]}}
\emph{{[}Initial interrater reliability was low, \(\kappa = 0.4\), with 50 out of 200 articles classified discordantly.{]}} To understand the disagreement, both authors examined the 50 discordant articles using an interactive grounded theory approach. After this process, some initial codings were changed and \emph{{[}almost{]}} all of the remaining \emph{{[}n{]}} discordant articles were coded as follows:

\begin{description}
\tightlist
\item[hereditarianism and/or eugenics]
29 papers. On the ``eugenics'' side, mostly examines or otherwise discusses the ``dysgenic'' claim that ``low-intelligence'' people have more children than ``high-intelligence'' people. The ``hereditarianism'' side is more varied, with papers on environmentalist interventions to raise IQ, genetics of IQ, and several other sub-topics.
\item[Flynn effect]
15 papers. Supporting, challenging, or otherwise focusing on Flynn's findings that IQ has increased over time.
\item[Lynn/national IQ]
3 papers. Supports, challenges, or focuses on international IQ data/comparisons made by Richard Lynn.
\end{description}

We agreed that, for all three categories of papers, there was reasonable ambiguity about whether they should be classified as race science discourse. These papers either do not explicitly mention race, or do so with just a brief mention of the ``race and intelligence controversy.'' On the other hand, they either contribute to or legitimatize areas of research that are deeply entangled with race science, and might thereby contribute to public perceptions of a legitimate scientific debate about race and intelligence.

Based on this qualitative review, we judged that the topic model approach was sufficiently sensitive (low ``false positive'' rate) in identifying race science discourse.

\bibliography{references.bib}
\bibliographystyle{Science}

% Following is a new environment, {scilastnote}, that's defined in the
% preamble and that allows authors to add a reference at the end of the
% list that's not signaled in the text; such references are used in
% *Science* for acknowledgments of funding, help, etc.

\begin{scilastnote}
\item \emph{{[}strutured{]}} Thanks to Emily Merchant and John Jackson for some initial discussions that helped clarify the scope of the project and identify some key resources related to the Pioneer Fund and \emph{Mankind Quarterly}. Thanks to Anthony Sainez for help retrieving the \emph{Mankind Quarterly} articles. Thanks to Derek Devnich and James Dooley for their work attempting to secure the APA-published articles. EL's work on this project was supported by UC Merced. DH's work on this project was not supported by any specific funding.
\end{scilastnote}


\renewcommand{\thetable}{\arabic{table}}
\renewcommand{\thefigure}{\arabic{figure}}
\setcounter{table}{0}
\setcounter{figure}{0}

%%%begfigs---

%%%endfigs---

%%%begtabs---

%%%endtabs---

% Needs a bit of work on the \ref{} TODO  before uncommenting
%\renewcommand{\thetable}{A\arabic{table}}
%\renewcommand{\thefigure}{A\arabic{figure}}
%\setcounter{table}{0}
%\setcounter{figure}{0}

%%%begappxfigs---

%%%endappxfigs---

%%%begappxtabs---

%%%endappxtabs---

\end{document}
