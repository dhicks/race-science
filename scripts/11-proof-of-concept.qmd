This section uses simulated data to provide a proof-of-concept for our use of topic models to detect the "mainstreaming" of a marginalized topic.  

```{r setup}
#| message: false
#| warning: false
library(tidyverse)
theme_set(theme_bw())
library(tmfast)

library(glue)
library(tictoc)
library(memoise)

set.seed(2024-03-18)

## Simulation parameters
k = 20            ## total number of topics
N = 100           ## papers per journal
vocab = 1000      ## vocabulary size
paper_size = 500  ## length of each paper
```

We construct a simulated corpus as follows.  We work with five journals, labelled A through E.  Journal A primarily publishes the marginalized topic (correspond to race science).  Ten journals, C-L, are mainstream journals, and publish an even mix from `r k-1` other topics, but avoid the marginalized topic (weighted 0 in the Dirichlet parameterization below).  These journals represent the context or comparison group for identifying the race science topic; their exact number is not important here.  

Journal B is the venue by which the marginalized topic is "mainstreamed."  This journal is represented in a time-indexed way, $B_t$, with $t$ ranging from 0 to 5.  For simplicity, in the text each time-slice of B is treated as a distinct journal, giving us 17 journals total (A; the ten C-L; plus 6 time-slices of B). 

This gives us three sets of journals: A, B (0-5), and C-L.  In the LDA model, these journal sets correspond to $\mathbf{\alpha}$, the parameter for the "prior" Dirichlet distribution $\Theta(\mathbf{\alpha})$. The `tmfast` package provides a helper function `peak_alpha()` for constructing $\mathbf{\alpha}$. 

```{r}
## Construct journal theta ----
## A: marginal topic journal
A_alpha = peak_alpha(k, 1, peak = .9)
A_alpha
## B: mainstream journal that picks up marginal topic over time
B_alpha = map(0:5, ~peak_alpha(k, 1, peak = 0.05*.x)) |> 
    set_names(glue('B{0:5}'))
## C-E: mainstream journals
C_alpha = rep.int(list(peak_alpha(k, 1, peak = 0)), 10) |> 
    set_names(LETTERS[3:12])
C_alpha$C

alphas = c(list(A = A_alpha), 
           B_alpha, 
           C_alpha)

## 17 journals/journals-at-time-t
length(alphas)
```

Each paper has a simplex $\theta \sim \Theta(\mathbf{\alpha})$, representing a distribution over topics. We draw `r N` papers from each journal. 

```{r}
theta = map(alphas, ~ rdirichlet(N, .x)) |> 
    imap(~ magrittr::set_rownames(.x, glue('{.y}-{str_pad(1:N, 
                                           2, 
                                           pad = "0")}'))) |> 
    reduce(rbind)
    
head(theta)

theta_df = theta |> 
    as_tibble(rownames = 'doc', 
              .name_repair = tmfast:::make_colnames) |> 
    pivot_longer(cols = -doc, 
                 names_to = 'topic', 
                 values_to = 'theta')

theta_df
```

We next define $\phi$, the distribution over words for each topic.  We use a vocabulary of `r vocab` words, and draw each topic's $\phi$ as a simplex from a symmetric Dirichlet distribution with $\alpha = 0.1$. Note that words are simply natural numbers. 

```{r}
## Word distributions ----
## 20 topics, 1000 terms, 
## uniform Dirichlet distribution w/ param 0.1
phi = rdirichlet(k, 0.1, k = vocab)
str(phi)
```

For simplicity, we use a uniform length of `r paper_size` words for each paper. `draw_corpus` assumes that this is a vector with one element for each document.   

```{r}
## Document length ----
## 500 words each
doc_len = rep.int(paper_size, N * length(alphas))
```

With these elements constructed, `draw_corpus()` generates a synthetic document-term matrix. Because this process is relatively slow (approximately 75 seconds on the laptop used to prepare this simulation), we use `memoise` to create a cache. As a pre-processing step for the topic model, we apply the `log(x+1)` transform. 

```{r}
## Draw corpus ----
draw_corpus_cached = memoise(draw_corpus, 
                              cache = cachem::cache_disk('draw_corpus_cache'))
set.seed(2024-03-25)  ## NB draw_corpus() is non-deterministic! 
tic()
corpus = draw_corpus_cached(doc_len, theta, phi) |> 
    mutate(doc = rownames(theta)[doc], 
           n = log1p(n))
toc()
corpus
```

We can now fit the topic model. 

```{r}
## Fit topic model ----
fitted = tmfast(corpus, c(10, 15, 20, 25, 30))
```

The tile plot indicates that (1) documents in journal A generally have a high probability for topic 1; journals C-E generally have a low probability; and journal B gradually has more articles in topic 1. 

```{r}
## Topics by journal ----
add_journal = function(x) {
    mutate(x, journal = str_extract(document, '^[A-Z]*[0-9]?'))
}

tidy(fitted, k = 20, matrix = 'gamma') |> 
    add_journal() |> 
    ggplot(aes(document, topic, fill = gamma)) +
    geom_tile() +
    facet_wrap(vars(journal), scales = 'free', 
               ncol = 7) +
    scale_x_discrete(guide = 'none')
```

Focusing on topic 1, we first calculate the average probability across each journal.  A has the highest value; C-E are consistently low; and B shows an increase over time. 

```{r}
## Topic model clearly detects the focal topic in A, increase in B over time, minimal presence in other journals
tidy(fitted, k = 20, matrix = 'gamma') |> 
    filter(topic == 'V01') |> 
    add_journal() |> 
    ggplot(aes(journal, gamma)) +
    ggforce::geom_sina() +
    stat_summary(color = 'red')
```

While the journals are largely distinguishable on the other topics, except for the unusually low value for A. 
```{r}
## With no pattern for the other topics
tidy(fitted, k = 20, matrix = 'gamma') |> 
    filter(topic == 'V02') |> 
    add_journal() |> 
    ggplot(aes(journal, gamma)) +
    ggforce::geom_sina() +
    stat_summary(color = 'red')
```

Next we calculate the rate of documents with probability greater than 0.1 in topic 1. As expected, this is almost all papers in journal A, no papers in the other mainstream journals, and in B the rate increases over time. 

```{r}
tidy(fitted, k = 20, matrix = 'gamma') |> 
    filter(topic == 'V01', 
           gamma > .1) |> 
    add_journal() |> 
    count(journal) |> 
    mutate(rate = n / N)
```

We therefore conclude that this topic model method can detect the "mainstreaming" from a marginal topic over time.  

As a limitation, we note that, for $\theta > 0.05$ or so, the estimated topic probabilities are below the true values. 
```{r}
tidy(fitted, k = 20, matrix = 'gamma') |> 
    filter(topic == 'V01') |> 
    add_journal() |> 
    inner_join(theta_df, by = c('document' = 'doc', 'topic')) |> 
    ggplot(aes(theta, gamma, color = journal)) +
    geom_point() +
    stat_function(fun = identity, 
                  inherit.aes = FALSE, color = 'black') +
    scale_color_viridis_d()
```

*[tmfast paper]* notes this problem, and implements a "renormalization" option when extracting the probability distributions from the fitted model.  This "renormalization" increases the "peakedness" of the distribution (lower entropy), adjusting the highest probabilities up and making the small probabilities several orders of magnitude smaller.  Renormalization would just amplify the differences found here, and so is not implemented for this proof-of-concept.  Renormalization is used in the primary, empirical analysis. 


